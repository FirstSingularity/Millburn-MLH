{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FirstSingularity/Millburn-MLH/blob/master/CMSC472_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Instructions\n",
        "\n",
        "1. Enter your Name, UID and Link to Google Drive in the provided space.\n",
        "2. Submit the assignment to Gradescope.\n",
        "\n",
        "\n",
        "Final Submission Deadline: May 2, 5:00pm\n",
        "\n",
        "Late Submission Deadline: May 4, 5:00pm"
      ],
      "metadata": {
        "id": "VYqASlKXhSf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:  **Neil Shah**  \n",
        "UID:  **117453591**\n",
        "\n",
        "Link to Google Drive : [LINK](https://drive.google.com/drive/folders/1r1STp4qIEmfLs-Pb7TSpLJn7ULN-BCTz?usp=sharing)"
      ],
      "metadata": {
        "id": "m3d1a2uihZGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will learn how to use transformers to generate text. Specifically, you will implement very small GPT model. It will predict streams of characters to attempt to form nice sounding sentences.\n",
        "\n",
        "You will complete 5 exercises, described in detail later on in this notebook."
      ],
      "metadata": {
        "id": "jk4V_YE-hZbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "EXspyciwR-8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION "
      ],
      "metadata": {
        "id": "XKLs5jsr-uSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tiny shakespeare dataset\n",
        "if not os.path.exists('data'):\n",
        "  os.makedirs('data')\n",
        "if not os.path.exists('data/shakespeare'):\n",
        "  os.makedirs('data/shakespeare')\n",
        "data_root = 'data/shakespeare'\n",
        "input_file_path = os.path.join(data_root, 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/learn2phoenix/CMSC472_HW6/main/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_root, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_root, 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(f'{data_root}/meta.pkl', 'wb') as f:\n",
        "    pickle.dump(meta, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0SfnEVkiV04",
        "outputId": "1f411f07-a210-44a2-81f3-83d40aa3948e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,395\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,855 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path"
      ],
      "metadata": {
        "id": "zfW-HWLs0FxX",
        "outputId": "6506080a-c687-4e12-f7f4-0a8c451c73f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/shakespeare/input.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Complete the TODO sections in the cell below."
      ],
      "metadata": {
        "id": "kg2abLjoAyrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n",
        "def new_gelu(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "5CrWG-UEWXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 1\n",
        "Complete the forward function for `CausalSelfAttention` class. Most of the function is already implemented, you just have to compute the query, key and values."
      ],
      "metadata": {
        "id": "NqK0wyxEggcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # TODO: you should calculate key, query, values (k, q, v) from `x` for all heads in batch.\n",
        "        # Don't forget to move head forward to be the batch dim\n",
        "        # HINT: using self.c_attn and splits to have q, k, v\n",
        "        # YOUR CODE BEGINS HERE\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # YOUR CODE ENDS HERE\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ],
      "metadata": {
        "id": "SxQvYKPQWZSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some other utility blocks are defined as:"
      ],
      "metadata": {
        "id": "uRoKHgUHkJa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = new_gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ],
      "metadata": {
        "id": "4CKrJM5MWdyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "Complete the forward function for `GPT` class. Most of the function is again already implemented, you need to do forward for `self.transformer` of this class. \n",
        "\n",
        "**HINT:** Read the token and position embeddings, forward through each block in loop and then forward through last layer."
      ],
      "metadata": {
        "id": "Zh1NjhlDgrLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        remember to subtract the position embeddings for non_embedding\n",
        "        The token embeddings would have received the same treatement too, but \n",
        "        for their use as weights, due to parameter sharing, in the final layer.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # TODO: write the forward for the GPT model and assign output to x. HINT: Refer to definition for self.transformer\n",
        "        # YOUR CODE BEGINS HERE\n",
        "        token_emb = self.transformer.wte(idx)\n",
        "        positional_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(token_emb + positional_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "        config_args['bias'] = True\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] \n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        decay.remove('lm_head.weight')\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)\n",
        "        flops_promised = 312e12 \n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "x0nmLFiFnKLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "FC8pX51nmXad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN CONFIG\n",
        "out_dir = 'out-shakespeare-char'\n",
        "eval_interval = 250\n",
        "log_interval = 10\n",
        "eval_iters = 200\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "# data\n",
        "dataset = 'shakespeare'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "bias =  False\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000 # 5000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 100\n",
        "lr_decay_iters = 5000 # 5000\n",
        "min_lr = 1e-4\n",
        "# system\n",
        "device = 'cuda'\n",
        "dtype = 'float16'\n",
        "compile = False\n",
        "\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZoStRV_mKNu",
        "outputId": "2d1cb153-5c51-4af3-f11a-d3dab850016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 \n",
        "1. Complete the TODO sections in the cell below and train the model on the shakespeare data. Complete the `get_lr` function. You should implement your learning rate schedule here. Your learning rate schedule should involve linear warmup and cosine decay.\n",
        "\n",
        "Train the model. For training, you should get loss below 2.0 in roughly 10 minutes. You should not need to run for any longer than 20 minutes (on colab GPU) to get nice results. If you're just testing things out, consider training for only a minute or so at a time, and just confirming that loss decreases. You should only need to train from start to finish 1 time- when you're ready to submit."
      ],
      "metadata": {
        "id": "QonpBseImY7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # TODO: Implement the learning rate schedule and return lr for the iteration\n",
        "    # 1: include linear warmup\n",
        "    # 2: implement cosine decay for after warmup (use warmup_iters from your hyperparams)\n",
        "    # YOUR CODE BEGINS HERE\n",
        "    if it < warmup_iters:\n",
        "        lr = learning_rate * (it / warmup_iters)\n",
        "        return lr\n",
        "    else:\n",
        "        progress = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "        decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
        "        coefficient = max(0, decay)\n",
        "        lr = min_lr + coefficient * (learning_rate - min_lr)\n",
        "        assert 0 <= decay <= 1\n",
        "    # YOUR CODE ENDS HERE\n",
        "        return min_lr + coefficient * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "UQb2FyyaR68U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)\n",
        "model_args['vocab_size'] = meta_vocab_size\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model\n",
        "running_mfu = -1.0\n",
        "for iter_num in range(max_iters):\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "    if grad_clip != 0.0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    local_iter_num += 1"
      ],
      "metadata": {
        "id": "EnoNsDhOiMi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73eff58-cf7c-446f-d1d5-1abb1440e0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found vocab_size = 65 (inside data/shakespeare/meta.pkl)\n",
            "number of parameters: 10.65M\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2875, val loss 4.2826\n",
            "iter 0: loss 4.2627, time 30271.88ms, mfu -100.00%\n",
            "iter 10: loss 3.4174, time 209.57ms, mfu 1.78%\n",
            "iter 20: loss 3.0873, time 210.31ms, mfu 1.78%\n",
            "iter 30: loss 2.7987, time 210.86ms, mfu 1.78%\n",
            "iter 40: loss 2.6393, time 210.23ms, mfu 1.78%\n",
            "iter 50: loss 2.5722, time 211.25ms, mfu 1.77%\n",
            "iter 60: loss 2.5403, time 211.63ms, mfu 1.77%\n",
            "iter 70: loss 2.5351, time 211.11ms, mfu 1.77%\n",
            "iter 80: loss 2.4992, time 211.69ms, mfu 1.77%\n",
            "iter 90: loss 2.4816, time 211.37ms, mfu 1.77%\n",
            "iter 100: loss 2.4825, time 212.18ms, mfu 1.77%\n",
            "iter 110: loss 2.4894, time 211.79ms, mfu 1.77%\n",
            "iter 120: loss 2.4637, time 211.82ms, mfu 1.77%\n",
            "iter 130: loss 2.4596, time 212.94ms, mfu 1.77%\n",
            "iter 140: loss 2.4591, time 212.67ms, mfu 1.76%\n",
            "iter 150: loss 2.4523, time 212.22ms, mfu 1.76%\n",
            "iter 160: loss 2.4573, time 211.95ms, mfu 1.76%\n",
            "iter 170: loss 2.4573, time 211.91ms, mfu 1.76%\n",
            "iter 180: loss 2.4456, time 212.53ms, mfu 1.76%\n",
            "iter 190: loss 2.4387, time 212.54ms, mfu 1.76%\n",
            "iter 200: loss 2.4346, time 212.68ms, mfu 1.76%\n",
            "iter 210: loss 2.4192, time 213.03ms, mfu 1.76%\n",
            "iter 220: loss 2.4110, time 213.15ms, mfu 1.76%\n",
            "iter 230: loss 2.4411, time 212.27ms, mfu 1.76%\n",
            "iter 240: loss 2.4202, time 213.85ms, mfu 1.76%\n",
            "step 250: train loss 2.3806, val loss 2.4053\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.4041, time 30696.70ms, mfu 1.58%\n",
            "iter 260: loss 2.4025, time 213.19ms, mfu 1.60%\n",
            "iter 270: loss 2.4099, time 213.89ms, mfu 1.61%\n",
            "iter 280: loss 2.4153, time 213.08ms, mfu 1.63%\n",
            "iter 290: loss 2.4015, time 214.51ms, mfu 1.64%\n",
            "iter 300: loss 2.3845, time 215.78ms, mfu 1.65%\n",
            "iter 310: loss 2.3837, time 212.84ms, mfu 1.66%\n",
            "iter 320: loss 2.3646, time 213.06ms, mfu 1.67%\n",
            "iter 330: loss 2.3602, time 213.05ms, mfu 1.67%\n",
            "iter 340: loss 2.3762, time 215.01ms, mfu 1.68%\n",
            "iter 350: loss 2.3548, time 216.49ms, mfu 1.68%\n",
            "iter 360: loss 2.3373, time 214.19ms, mfu 1.69%\n",
            "iter 370: loss 2.3449, time 214.26ms, mfu 1.69%\n",
            "iter 380: loss 2.3099, time 213.00ms, mfu 1.70%\n",
            "iter 390: loss 2.3363, time 213.34ms, mfu 1.70%\n",
            "iter 400: loss 2.3416, time 214.36ms, mfu 1.71%\n",
            "iter 410: loss 2.2887, time 216.61ms, mfu 1.71%\n",
            "iter 420: loss 2.3093, time 215.76ms, mfu 1.71%\n",
            "iter 430: loss 2.2626, time 216.44ms, mfu 1.71%\n",
            "iter 440: loss 2.2449, time 215.62ms, mfu 1.71%\n",
            "iter 450: loss 2.2597, time 216.26ms, mfu 1.71%\n",
            "iter 460: loss 2.1942, time 217.88ms, mfu 1.71%\n",
            "iter 470: loss 2.2230, time 215.38ms, mfu 1.72%\n",
            "iter 480: loss 2.2122, time 214.98ms, mfu 1.72%\n",
            "iter 490: loss 2.1659, time 214.35ms, mfu 1.72%\n",
            "step 500: train loss 2.0805, val loss 2.1627\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 2.1583, time 30786.90ms, mfu 1.55%\n",
            "iter 510: loss 2.1499, time 216.83ms, mfu 1.57%\n",
            "iter 520: loss 2.1330, time 215.19ms, mfu 1.58%\n",
            "iter 530: loss 2.1305, time 217.56ms, mfu 1.60%\n",
            "iter 540: loss 2.1444, time 215.77ms, mfu 1.61%\n",
            "iter 550: loss 2.1081, time 217.02ms, mfu 1.62%\n",
            "iter 560: loss 2.1302, time 216.96ms, mfu 1.63%\n",
            "iter 570: loss 2.1040, time 214.25ms, mfu 1.64%\n",
            "iter 580: loss 2.0712, time 215.94ms, mfu 1.65%\n",
            "iter 590: loss 2.0616, time 217.08ms, mfu 1.66%\n",
            "iter 600: loss 2.0446, time 217.43ms, mfu 1.66%\n",
            "iter 610: loss 2.0206, time 218.76ms, mfu 1.67%\n",
            "iter 620: loss 2.0166, time 216.11ms, mfu 1.67%\n",
            "iter 630: loss 2.0188, time 214.08ms, mfu 1.68%\n",
            "iter 640: loss 1.9879, time 214.78ms, mfu 1.68%\n",
            "iter 650: loss 1.9960, time 213.51ms, mfu 1.69%\n",
            "iter 660: loss 1.9907, time 215.19ms, mfu 1.69%\n",
            "iter 670: loss 1.9341, time 214.27ms, mfu 1.70%\n",
            "iter 680: loss 1.9505, time 213.17ms, mfu 1.70%\n",
            "iter 690: loss 1.9437, time 213.90ms, mfu 1.71%\n",
            "iter 700: loss 1.9105, time 214.51ms, mfu 1.71%\n",
            "iter 710: loss 1.9005, time 213.57ms, mfu 1.71%\n",
            "iter 720: loss 1.8695, time 214.30ms, mfu 1.72%\n",
            "iter 730: loss 1.8979, time 217.80ms, mfu 1.72%\n",
            "iter 740: loss 1.8795, time 215.12ms, mfu 1.72%\n",
            "step 750: train loss 1.7712, val loss 1.9243\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.8569, time 31006.61ms, mfu 1.55%\n",
            "iter 760: loss 1.8790, time 218.13ms, mfu 1.56%\n",
            "iter 770: loss 1.8534, time 217.22ms, mfu 1.58%\n",
            "iter 780: loss 1.8384, time 213.53ms, mfu 1.59%\n",
            "iter 790: loss 1.8409, time 218.57ms, mfu 1.61%\n",
            "iter 800: loss 1.8510, time 228.61ms, mfu 1.61%\n",
            "iter 810: loss 1.8115, time 218.66ms, mfu 1.62%\n",
            "iter 820: loss 1.8149, time 219.18ms, mfu 1.63%\n",
            "iter 830: loss 1.7781, time 214.48ms, mfu 1.64%\n",
            "iter 840: loss 1.7811, time 218.59ms, mfu 1.64%\n",
            "iter 850: loss 1.7588, time 214.49ms, mfu 1.65%\n",
            "iter 860: loss 1.7484, time 217.10ms, mfu 1.66%\n",
            "iter 870: loss 1.7725, time 216.97ms, mfu 1.67%\n",
            "iter 880: loss 1.7485, time 215.37ms, mfu 1.67%\n",
            "iter 890: loss 1.7209, time 214.67ms, mfu 1.68%\n",
            "iter 900: loss 1.7418, time 214.40ms, mfu 1.68%\n",
            "iter 910: loss 1.7512, time 220.49ms, mfu 1.68%\n",
            "iter 920: loss 1.7371, time 218.13ms, mfu 1.69%\n",
            "iter 930: loss 1.7401, time 215.66ms, mfu 1.69%\n",
            "iter 940: loss 1.7175, time 214.31ms, mfu 1.70%\n",
            "iter 950: loss 1.7065, time 217.73ms, mfu 1.70%\n",
            "iter 960: loss 1.7037, time 218.39ms, mfu 1.70%\n",
            "iter 970: loss 1.6881, time 215.10ms, mfu 1.70%\n",
            "iter 980: loss 1.6826, time 218.59ms, mfu 1.70%\n",
            "iter 990: loss 1.6733, time 215.26ms, mfu 1.70%\n",
            "step 1000: train loss 1.5805, val loss 1.7600\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.6926, time 31076.16ms, mfu 1.54%\n",
            "iter 1010: loss 1.6874, time 219.22ms, mfu 1.55%\n",
            "iter 1020: loss 1.6313, time 214.79ms, mfu 1.57%\n",
            "iter 1030: loss 1.6634, time 218.32ms, mfu 1.58%\n",
            "iter 1040: loss 1.6463, time 218.32ms, mfu 1.60%\n",
            "iter 1050: loss 1.6699, time 220.70ms, mfu 1.61%\n",
            "iter 1060: loss 1.6326, time 214.80ms, mfu 1.62%\n",
            "iter 1070: loss 1.6456, time 215.80ms, mfu 1.63%\n",
            "iter 1080: loss 1.6742, time 219.61ms, mfu 1.64%\n",
            "iter 1090: loss 1.6606, time 215.45ms, mfu 1.65%\n",
            "iter 1100: loss 1.5903, time 220.05ms, mfu 1.65%\n",
            "iter 1110: loss 1.6144, time 215.77ms, mfu 1.66%\n",
            "iter 1120: loss 1.5888, time 216.82ms, mfu 1.66%\n",
            "iter 1130: loss 1.6376, time 217.81ms, mfu 1.67%\n",
            "iter 1140: loss 1.6063, time 219.36ms, mfu 1.67%\n",
            "iter 1150: loss 1.6031, time 218.10ms, mfu 1.68%\n",
            "iter 1160: loss 1.6091, time 216.45ms, mfu 1.68%\n",
            "iter 1170: loss 1.5839, time 218.78ms, mfu 1.68%\n",
            "iter 1180: loss 1.6057, time 218.19ms, mfu 1.68%\n",
            "iter 1190: loss 1.5581, time 218.60ms, mfu 1.69%\n",
            "iter 1200: loss 1.5938, time 217.52ms, mfu 1.69%\n",
            "iter 1210: loss 1.5814, time 217.96ms, mfu 1.69%\n",
            "iter 1220: loss 1.5847, time 218.91ms, mfu 1.69%\n",
            "iter 1230: loss 1.5774, time 218.80ms, mfu 1.69%\n",
            "iter 1240: loss 1.5728, time 215.57ms, mfu 1.70%\n",
            "step 1250: train loss 1.4818, val loss 1.6828\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.5674, time 31097.34ms, mfu 1.53%\n",
            "iter 1260: loss 1.5724, time 216.79ms, mfu 1.55%\n",
            "iter 1270: loss 1.5588, time 218.96ms, mfu 1.56%\n",
            "iter 1280: loss 1.5557, time 214.40ms, mfu 1.58%\n",
            "iter 1290: loss 1.5649, time 218.72ms, mfu 1.59%\n",
            "iter 1300: loss 1.5538, time 218.22ms, mfu 1.60%\n",
            "iter 1310: loss 1.5660, time 219.28ms, mfu 1.61%\n",
            "iter 1320: loss 1.5955, time 217.27ms, mfu 1.62%\n",
            "iter 1330: loss 1.5337, time 219.37ms, mfu 1.63%\n",
            "iter 1340: loss 1.5850, time 214.24ms, mfu 1.64%\n",
            "iter 1350: loss 1.5508, time 216.77ms, mfu 1.65%\n",
            "iter 1360: loss 1.5115, time 215.62ms, mfu 1.66%\n",
            "iter 1370: loss 1.5312, time 219.65ms, mfu 1.66%\n",
            "iter 1380: loss 1.5067, time 220.76ms, mfu 1.66%\n",
            "iter 1390: loss 1.5280, time 221.99ms, mfu 1.67%\n",
            "iter 1400: loss 1.5103, time 221.08ms, mfu 1.67%\n",
            "iter 1410: loss 1.5078, time 216.35ms, mfu 1.67%\n",
            "iter 1420: loss 1.4949, time 217.05ms, mfu 1.68%\n",
            "iter 1430: loss 1.5239, time 216.35ms, mfu 1.68%\n",
            "iter 1440: loss 1.4740, time 218.18ms, mfu 1.68%\n",
            "iter 1450: loss 1.5152, time 217.83ms, mfu 1.69%\n",
            "iter 1460: loss 1.5023, time 217.32ms, mfu 1.69%\n",
            "iter 1470: loss 1.5059, time 216.80ms, mfu 1.69%\n",
            "iter 1480: loss 1.4876, time 213.67ms, mfu 1.70%\n",
            "iter 1490: loss 1.5238, time 217.99ms, mfu 1.70%\n",
            "step 1500: train loss 1.3989, val loss 1.6131\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.5128, time 31105.29ms, mfu 1.53%\n",
            "iter 1510: loss 1.4798, time 217.92ms, mfu 1.55%\n",
            "iter 1520: loss 1.4671, time 215.22ms, mfu 1.57%\n",
            "iter 1530: loss 1.4952, time 217.89ms, mfu 1.58%\n",
            "iter 1540: loss 1.5183, time 217.61ms, mfu 1.59%\n",
            "iter 1550: loss 1.4620, time 218.58ms, mfu 1.61%\n",
            "iter 1560: loss 1.4914, time 214.60ms, mfu 1.62%\n",
            "iter 1570: loss 1.4605, time 216.72ms, mfu 1.63%\n",
            "iter 1580: loss 1.4756, time 215.28ms, mfu 1.64%\n",
            "iter 1590: loss 1.4519, time 218.84ms, mfu 1.65%\n",
            "iter 1600: loss 1.4717, time 217.17ms, mfu 1.65%\n",
            "iter 1610: loss 1.4528, time 208.53ms, mfu 1.67%\n",
            "iter 1620: loss 1.4361, time 225.89ms, mfu 1.66%\n",
            "iter 1630: loss 1.4407, time 217.24ms, mfu 1.67%\n",
            "iter 1640: loss 1.4798, time 219.57ms, mfu 1.67%\n",
            "iter 1650: loss 1.4501, time 215.14ms, mfu 1.68%\n",
            "iter 1660: loss 1.4270, time 218.48ms, mfu 1.68%\n",
            "iter 1670: loss 1.4516, time 216.84ms, mfu 1.68%\n",
            "iter 1680: loss 1.4211, time 216.21ms, mfu 1.69%\n",
            "iter 1690: loss 1.4397, time 215.27ms, mfu 1.69%\n",
            "iter 1700: loss 1.4865, time 219.13ms, mfu 1.69%\n",
            "iter 1710: loss 1.4158, time 217.61ms, mfu 1.70%\n",
            "iter 1720: loss 1.4442, time 219.67ms, mfu 1.70%\n",
            "iter 1730: loss 1.4030, time 215.85ms, mfu 1.70%\n",
            "iter 1740: loss 1.3913, time 218.84ms, mfu 1.70%\n",
            "step 1750: train loss 1.3454, val loss 1.5730\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.4212, time 30959.06ms, mfu 1.53%\n",
            "iter 1760: loss 1.4411, time 218.87ms, mfu 1.55%\n",
            "iter 1770: loss 1.4309, time 214.00ms, mfu 1.57%\n",
            "iter 1780: loss 1.4443, time 218.43ms, mfu 1.58%\n",
            "iter 1790: loss 1.3826, time 219.14ms, mfu 1.59%\n",
            "iter 1800: loss 1.4488, time 218.08ms, mfu 1.60%\n",
            "iter 1810: loss 1.4163, time 220.04ms, mfu 1.61%\n",
            "iter 1820: loss 1.4329, time 217.92ms, mfu 1.62%\n",
            "iter 1830: loss 1.4294, time 220.79ms, mfu 1.63%\n",
            "iter 1840: loss 1.4227, time 216.39ms, mfu 1.64%\n",
            "iter 1850: loss 1.4387, time 219.68ms, mfu 1.64%\n",
            "iter 1860: loss 1.4365, time 214.16ms, mfu 1.65%\n",
            "iter 1870: loss 1.3982, time 214.75ms, mfu 1.66%\n",
            "iter 1880: loss 1.4020, time 217.17ms, mfu 1.67%\n",
            "iter 1890: loss 1.4052, time 219.27ms, mfu 1.67%\n",
            "iter 1900: loss 1.3903, time 218.23ms, mfu 1.67%\n",
            "iter 1910: loss 1.3799, time 214.23ms, mfu 1.68%\n",
            "iter 1920: loss 1.3968, time 217.95ms, mfu 1.68%\n",
            "iter 1930: loss 1.4128, time 218.27ms, mfu 1.69%\n",
            "iter 1940: loss 1.3855, time 218.61ms, mfu 1.69%\n",
            "iter 1950: loss 1.4161, time 224.19ms, mfu 1.69%\n",
            "iter 1960: loss 1.3920, time 227.28ms, mfu 1.68%\n",
            "iter 1970: loss 1.3957, time 217.45ms, mfu 1.68%\n",
            "iter 1980: loss 1.3966, time 220.32ms, mfu 1.68%\n",
            "iter 1990: loss 1.3960, time 215.63ms, mfu 1.69%\n",
            "step 2000: train loss 1.3044, val loss 1.5412\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.3984, time 30954.92ms, mfu 1.52%\n",
            "iter 2010: loss 1.3811, time 217.32ms, mfu 1.54%\n",
            "iter 2020: loss 1.3774, time 218.00ms, mfu 1.56%\n",
            "iter 2030: loss 1.3789, time 216.60ms, mfu 1.57%\n",
            "iter 2040: loss 1.3472, time 217.72ms, mfu 1.59%\n",
            "iter 2050: loss 1.3338, time 216.69ms, mfu 1.60%\n",
            "iter 2060: loss 1.3743, time 218.15ms, mfu 1.61%\n",
            "iter 2070: loss 1.3547, time 217.30ms, mfu 1.62%\n",
            "iter 2080: loss 1.3611, time 214.83ms, mfu 1.63%\n",
            "iter 2090: loss 1.3688, time 216.11ms, mfu 1.64%\n",
            "iter 2100: loss 1.3699, time 217.23ms, mfu 1.65%\n",
            "iter 2110: loss 1.3193, time 218.17ms, mfu 1.66%\n",
            "iter 2120: loss 1.3705, time 217.41ms, mfu 1.66%\n",
            "iter 2130: loss 1.3830, time 214.62ms, mfu 1.67%\n",
            "iter 2140: loss 1.3324, time 218.30ms, mfu 1.67%\n",
            "iter 2150: loss 1.3199, time 215.07ms, mfu 1.68%\n",
            "iter 2160: loss 1.3302, time 218.05ms, mfu 1.68%\n",
            "iter 2170: loss 1.3451, time 216.68ms, mfu 1.69%\n",
            "iter 2180: loss 1.3461, time 216.51ms, mfu 1.69%\n",
            "iter 2190: loss 1.3518, time 216.94ms, mfu 1.69%\n",
            "iter 2200: loss 1.3491, time 219.48ms, mfu 1.69%\n",
            "iter 2210: loss 1.3614, time 216.77ms, mfu 1.70%\n",
            "iter 2220: loss 1.3152, time 219.38ms, mfu 1.70%\n",
            "iter 2230: loss 1.3502, time 216.76ms, mfu 1.70%\n",
            "iter 2240: loss 1.3659, time 214.87ms, mfu 1.70%\n",
            "step 2250: train loss 1.2584, val loss 1.5149\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.3132, time 30960.98ms, mfu 1.53%\n",
            "iter 2260: loss 1.3645, time 219.03ms, mfu 1.55%\n",
            "iter 2270: loss 1.3306, time 215.13ms, mfu 1.57%\n",
            "iter 2280: loss 1.3585, time 217.36ms, mfu 1.58%\n",
            "iter 2290: loss 1.3631, time 218.45ms, mfu 1.59%\n",
            "iter 2300: loss 1.3382, time 217.66ms, mfu 1.61%\n",
            "iter 2310: loss 1.3166, time 218.31ms, mfu 1.62%\n",
            "iter 2320: loss 1.3527, time 219.30ms, mfu 1.62%\n",
            "iter 2330: loss 1.2931, time 218.77ms, mfu 1.63%\n",
            "iter 2340: loss 1.3575, time 218.63ms, mfu 1.64%\n",
            "iter 2350: loss 1.3130, time 215.06ms, mfu 1.65%\n",
            "iter 2360: loss 1.3436, time 216.85ms, mfu 1.66%\n",
            "iter 2370: loss 1.3280, time 218.07ms, mfu 1.66%\n",
            "iter 2380: loss 1.3339, time 216.44ms, mfu 1.67%\n",
            "iter 2390: loss 1.3457, time 218.07ms, mfu 1.67%\n",
            "iter 2400: loss 1.3400, time 217.92ms, mfu 1.68%\n",
            "iter 2410: loss 1.3368, time 216.92ms, mfu 1.68%\n",
            "iter 2420: loss 1.3255, time 216.09ms, mfu 1.68%\n",
            "iter 2430: loss 1.3311, time 217.41ms, mfu 1.69%\n",
            "iter 2440: loss 1.3388, time 217.80ms, mfu 1.69%\n",
            "iter 2450: loss 1.3137, time 215.06ms, mfu 1.69%\n",
            "iter 2460: loss 1.3046, time 218.39ms, mfu 1.69%\n",
            "iter 2470: loss 1.3319, time 219.87ms, mfu 1.69%\n",
            "iter 2480: loss 1.3440, time 216.34ms, mfu 1.70%\n",
            "iter 2490: loss 1.3428, time 218.36ms, mfu 1.70%\n",
            "step 2500: train loss 1.2369, val loss 1.5041\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.3318, time 30932.04ms, mfu 1.53%\n",
            "iter 2510: loss 1.3435, time 216.39ms, mfu 1.55%\n",
            "iter 2520: loss 1.3100, time 217.95ms, mfu 1.57%\n",
            "iter 2530: loss 1.3150, time 214.82ms, mfu 1.58%\n",
            "iter 2540: loss 1.3043, time 214.53ms, mfu 1.60%\n",
            "iter 2550: loss 1.3071, time 218.79ms, mfu 1.61%\n",
            "iter 2560: loss 1.3272, time 217.82ms, mfu 1.62%\n",
            "iter 2570: loss 1.3433, time 215.85ms, mfu 1.63%\n",
            "iter 2580: loss 1.3076, time 218.02ms, mfu 1.64%\n",
            "iter 2590: loss 1.3289, time 216.41ms, mfu 1.65%\n",
            "iter 2600: loss 1.3274, time 218.89ms, mfu 1.65%\n",
            "iter 2610: loss 1.3021, time 214.18ms, mfu 1.66%\n",
            "iter 2620: loss 1.3057, time 218.08ms, mfu 1.67%\n",
            "iter 2630: loss 1.2981, time 218.62ms, mfu 1.67%\n",
            "iter 2640: loss 1.3204, time 216.36ms, mfu 1.67%\n",
            "iter 2650: loss 1.3042, time 218.61ms, mfu 1.68%\n",
            "iter 2660: loss 1.3213, time 216.71ms, mfu 1.68%\n",
            "iter 2670: loss 1.3242, time 218.11ms, mfu 1.68%\n",
            "iter 2680: loss 1.3065, time 217.92ms, mfu 1.69%\n",
            "iter 2690: loss 1.3105, time 218.48ms, mfu 1.69%\n",
            "iter 2700: loss 1.2820, time 215.25ms, mfu 1.69%\n",
            "iter 2710: loss 1.3096, time 216.67ms, mfu 1.70%\n",
            "iter 2720: loss 1.3191, time 219.58ms, mfu 1.70%\n",
            "iter 2730: loss 1.2970, time 219.32ms, mfu 1.70%\n",
            "iter 2740: loss 1.3019, time 218.75ms, mfu 1.70%\n",
            "step 2750: train loss 1.2176, val loss 1.4907\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.2936, time 30937.82ms, mfu 1.53%\n",
            "iter 2760: loss 1.3226, time 218.44ms, mfu 1.55%\n",
            "iter 2770: loss 1.2921, time 214.49ms, mfu 1.57%\n",
            "iter 2780: loss 1.2924, time 214.96ms, mfu 1.58%\n",
            "iter 2790: loss 1.2874, time 217.17ms, mfu 1.60%\n",
            "iter 2800: loss 1.2775, time 216.62ms, mfu 1.61%\n",
            "iter 2810: loss 1.2864, time 214.72ms, mfu 1.62%\n",
            "iter 2820: loss 1.2669, time 220.36ms, mfu 1.63%\n",
            "iter 2830: loss 1.3328, time 214.41ms, mfu 1.64%\n",
            "iter 2840: loss 1.2971, time 218.33ms, mfu 1.65%\n",
            "iter 2850: loss 1.2783, time 216.17ms, mfu 1.65%\n",
            "iter 2860: loss 1.2942, time 218.32ms, mfu 1.66%\n",
            "iter 2870: loss 1.2837, time 217.21ms, mfu 1.66%\n",
            "iter 2880: loss 1.3296, time 219.56ms, mfu 1.67%\n",
            "iter 2890: loss 1.3160, time 217.39ms, mfu 1.67%\n",
            "iter 2900: loss 1.3027, time 216.74ms, mfu 1.68%\n",
            "iter 2910: loss 1.3251, time 218.60ms, mfu 1.68%\n",
            "iter 2920: loss 1.2926, time 215.96ms, mfu 1.68%\n",
            "iter 2930: loss 1.2830, time 214.82ms, mfu 1.69%\n",
            "iter 2940: loss 1.2750, time 216.18ms, mfu 1.69%\n",
            "iter 2950: loss 1.2949, time 214.26ms, mfu 1.70%\n",
            "iter 2960: loss 1.2948, time 217.99ms, mfu 1.70%\n",
            "iter 2970: loss 1.2746, time 214.69ms, mfu 1.70%\n",
            "iter 2980: loss 1.2921, time 215.54ms, mfu 1.70%\n",
            "iter 2990: loss 1.2853, time 219.74ms, mfu 1.70%\n",
            "step 3000: train loss 1.1984, val loss 1.4802\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.2859, time 30924.70ms, mfu 1.53%\n",
            "iter 3010: loss 1.2680, time 216.81ms, mfu 1.55%\n",
            "iter 3020: loss 1.2965, time 218.11ms, mfu 1.57%\n",
            "iter 3030: loss 1.2817, time 215.95ms, mfu 1.58%\n",
            "iter 3040: loss 1.2979, time 219.40ms, mfu 1.60%\n",
            "iter 3050: loss 1.2652, time 218.63ms, mfu 1.61%\n",
            "iter 3060: loss 1.2557, time 216.91ms, mfu 1.62%\n",
            "iter 3070: loss 1.3164, time 220.05ms, mfu 1.63%\n",
            "iter 3080: loss 1.2435, time 216.35ms, mfu 1.64%\n",
            "iter 3090: loss 1.2549, time 217.33ms, mfu 1.64%\n",
            "iter 3100: loss 1.2661, time 219.86ms, mfu 1.65%\n",
            "iter 3110: loss 1.2921, time 214.29ms, mfu 1.66%\n",
            "iter 3120: loss 1.2645, time 216.31ms, mfu 1.66%\n",
            "iter 3130: loss 1.2689, time 214.86ms, mfu 1.67%\n",
            "iter 3140: loss 1.2674, time 218.12ms, mfu 1.67%\n",
            "iter 3150: loss 1.2723, time 217.76ms, mfu 1.68%\n",
            "iter 3160: loss 1.2669, time 217.37ms, mfu 1.68%\n",
            "iter 3170: loss 1.2757, time 220.49ms, mfu 1.68%\n",
            "iter 3180: loss 1.2861, time 220.15ms, mfu 1.68%\n",
            "iter 3190: loss 1.2876, time 216.85ms, mfu 1.69%\n",
            "iter 3200: loss 1.2631, time 216.87ms, mfu 1.69%\n",
            "iter 3210: loss 1.2499, time 214.27ms, mfu 1.70%\n",
            "iter 3220: loss 1.2854, time 217.54ms, mfu 1.70%\n",
            "iter 3230: loss 1.2715, time 215.18ms, mfu 1.70%\n",
            "iter 3240: loss 1.2404, time 218.79ms, mfu 1.70%\n",
            "step 3250: train loss 1.1834, val loss 1.4842\n",
            "iter 3250: loss 1.2545, time 30666.40ms, mfu 1.53%\n",
            "iter 3260: loss 1.2486, time 218.64ms, mfu 1.55%\n",
            "iter 3270: loss 1.2617, time 215.38ms, mfu 1.57%\n",
            "iter 3280: loss 1.2726, time 217.50ms, mfu 1.58%\n",
            "iter 3290: loss 1.2787, time 217.71ms, mfu 1.59%\n",
            "iter 3300: loss 1.2805, time 217.64ms, mfu 1.61%\n",
            "iter 3310: loss 1.2703, time 218.15ms, mfu 1.62%\n",
            "iter 3320: loss 1.2659, time 218.81ms, mfu 1.63%\n",
            "iter 3330: loss 1.2584, time 218.75ms, mfu 1.63%\n",
            "iter 3340: loss 1.2414, time 219.84ms, mfu 1.64%\n",
            "iter 3350: loss 1.2812, time 215.91ms, mfu 1.65%\n",
            "iter 3360: loss 1.2802, time 217.74ms, mfu 1.65%\n",
            "iter 3370: loss 1.2842, time 219.55ms, mfu 1.66%\n",
            "iter 3380: loss 1.3035, time 219.34ms, mfu 1.66%\n",
            "iter 3390: loss 1.2712, time 215.36ms, mfu 1.67%\n",
            "iter 3400: loss 1.2844, time 219.00ms, mfu 1.67%\n",
            "iter 3410: loss 1.2502, time 215.02ms, mfu 1.68%\n",
            "iter 3420: loss 1.2281, time 217.38ms, mfu 1.68%\n",
            "iter 3430: loss 1.2545, time 218.46ms, mfu 1.68%\n",
            "iter 3440: loss 1.2563, time 214.31ms, mfu 1.69%\n",
            "iter 3450: loss 1.2663, time 214.60ms, mfu 1.69%\n",
            "iter 3460: loss 1.2605, time 217.90ms, mfu 1.70%\n",
            "iter 3470: loss 1.2420, time 215.84ms, mfu 1.70%\n",
            "iter 3480: loss 1.2470, time 219.27ms, mfu 1.70%\n",
            "iter 3490: loss 1.2344, time 214.95ms, mfu 1.70%\n",
            "step 3500: train loss 1.1692, val loss 1.4693\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3500: loss 1.2606, time 30947.70ms, mfu 1.53%\n",
            "iter 3510: loss 1.2294, time 216.18ms, mfu 1.55%\n",
            "iter 3520: loss 1.2637, time 216.85ms, mfu 1.57%\n",
            "iter 3530: loss 1.2834, time 214.07ms, mfu 1.59%\n",
            "iter 3540: loss 1.2316, time 218.94ms, mfu 1.60%\n",
            "iter 3550: loss 1.2638, time 217.95ms, mfu 1.61%\n",
            "iter 3560: loss 1.2520, time 216.16ms, mfu 1.62%\n",
            "iter 3570: loss 1.2647, time 218.11ms, mfu 1.63%\n",
            "iter 3580: loss 1.2199, time 216.21ms, mfu 1.64%\n",
            "iter 3590: loss 1.2662, time 220.18ms, mfu 1.64%\n",
            "iter 3600: loss 1.2518, time 214.80ms, mfu 1.65%\n",
            "iter 3610: loss 1.2813, time 217.74ms, mfu 1.66%\n",
            "iter 3620: loss 1.2390, time 216.29ms, mfu 1.67%\n",
            "iter 3630: loss 1.2267, time 218.16ms, mfu 1.67%\n",
            "iter 3640: loss 1.2374, time 217.42ms, mfu 1.67%\n",
            "iter 3650: loss 1.2676, time 214.77ms, mfu 1.68%\n",
            "iter 3660: loss 1.2570, time 217.86ms, mfu 1.68%\n",
            "iter 3670: loss 1.2337, time 215.10ms, mfu 1.69%\n",
            "iter 3680: loss 1.2306, time 217.94ms, mfu 1.69%\n",
            "iter 3690: loss 1.2415, time 217.22ms, mfu 1.69%\n",
            "iter 3700: loss 1.2567, time 217.87ms, mfu 1.69%\n",
            "iter 3710: loss 1.2848, time 219.11ms, mfu 1.70%\n",
            "iter 3720: loss 1.2564, time 215.68ms, mfu 1.70%\n",
            "iter 3730: loss 1.2368, time 217.22ms, mfu 1.70%\n",
            "iter 3740: loss 1.2485, time 218.21ms, mfu 1.70%\n",
            "step 3750: train loss 1.1580, val loss 1.4691\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3750: loss 1.2550, time 30969.81ms, mfu 1.53%\n",
            "iter 3760: loss 1.2509, time 218.15ms, mfu 1.55%\n",
            "iter 3770: loss 1.2420, time 220.02ms, mfu 1.56%\n",
            "iter 3780: loss 1.2415, time 218.42ms, mfu 1.58%\n",
            "iter 3790: loss 1.2482, time 218.16ms, mfu 1.59%\n",
            "iter 3800: loss 1.2569, time 217.46ms, mfu 1.60%\n",
            "iter 3810: loss 1.2714, time 214.81ms, mfu 1.62%\n",
            "iter 3820: loss 1.2367, time 219.05ms, mfu 1.62%\n",
            "iter 3830: loss 1.2592, time 216.33ms, mfu 1.63%\n",
            "iter 3840: loss 1.2394, time 217.48ms, mfu 1.64%\n",
            "iter 3850: loss 1.2309, time 217.00ms, mfu 1.65%\n",
            "iter 3860: loss 1.2199, time 218.70ms, mfu 1.66%\n",
            "iter 3870: loss 1.2195, time 217.91ms, mfu 1.66%\n",
            "iter 3880: loss 1.2255, time 217.47ms, mfu 1.67%\n",
            "iter 3890: loss 1.2531, time 217.52ms, mfu 1.67%\n",
            "iter 3900: loss 1.2309, time 214.92ms, mfu 1.68%\n",
            "iter 3910: loss 1.2527, time 218.08ms, mfu 1.68%\n",
            "iter 3920: loss 1.2364, time 216.03ms, mfu 1.68%\n",
            "iter 3930: loss 1.2127, time 218.11ms, mfu 1.69%\n",
            "iter 3940: loss 1.2377, time 216.76ms, mfu 1.69%\n",
            "iter 3950: loss 1.2487, time 215.25ms, mfu 1.69%\n",
            "iter 3960: loss 1.2351, time 216.60ms, mfu 1.70%\n",
            "iter 3970: loss 1.2399, time 216.83ms, mfu 1.70%\n",
            "iter 3980: loss 1.2490, time 216.82ms, mfu 1.70%\n",
            "iter 3990: loss 1.2342, time 217.28ms, mfu 1.70%\n",
            "step 4000: train loss 1.1445, val loss 1.4692\n",
            "iter 4000: loss 1.2156, time 30686.87ms, mfu 1.53%\n",
            "iter 4010: loss 1.2319, time 218.07ms, mfu 1.55%\n",
            "iter 4020: loss 1.2270, time 217.88ms, mfu 1.57%\n",
            "iter 4030: loss 1.2230, time 220.00ms, mfu 1.58%\n",
            "iter 4040: loss 1.2317, time 217.49ms, mfu 1.59%\n",
            "iter 4050: loss 1.2337, time 215.37ms, mfu 1.61%\n",
            "iter 4060: loss 1.2218, time 220.16ms, mfu 1.62%\n",
            "iter 4070: loss 1.2203, time 218.57ms, mfu 1.62%\n",
            "iter 4080: loss 1.2136, time 221.56ms, mfu 1.63%\n",
            "iter 4090: loss 1.2309, time 217.96ms, mfu 1.64%\n",
            "iter 4100: loss 1.2101, time 217.04ms, mfu 1.65%\n",
            "iter 4110: loss 1.2657, time 218.42ms, mfu 1.65%\n",
            "iter 4120: loss 1.2419, time 215.00ms, mfu 1.66%\n",
            "iter 4130: loss 1.2399, time 217.46ms, mfu 1.67%\n",
            "iter 4140: loss 1.2507, time 215.31ms, mfu 1.67%\n",
            "iter 4150: loss 1.2312, time 214.55ms, mfu 1.68%\n",
            "iter 4160: loss 1.2214, time 219.23ms, mfu 1.68%\n",
            "iter 4170: loss 1.2180, time 215.98ms, mfu 1.68%\n",
            "iter 4180: loss 1.2190, time 218.50ms, mfu 1.69%\n",
            "iter 4190: loss 1.2583, time 220.71ms, mfu 1.69%\n",
            "iter 4200: loss 1.1977, time 217.28ms, mfu 1.69%\n",
            "iter 4210: loss 1.2059, time 220.61ms, mfu 1.69%\n",
            "iter 4220: loss 1.2419, time 217.58ms, mfu 1.69%\n",
            "iter 4230: loss 1.2336, time 217.86ms, mfu 1.69%\n",
            "iter 4240: loss 1.2341, time 214.71ms, mfu 1.70%\n",
            "step 4250: train loss 1.1348, val loss 1.4646\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 4250: loss 1.2402, time 30972.42ms, mfu 1.53%\n",
            "iter 4260: loss 1.1845, time 219.15ms, mfu 1.55%\n",
            "iter 4270: loss 1.2432, time 219.70ms, mfu 1.56%\n",
            "iter 4280: loss 1.2187, time 217.75ms, mfu 1.58%\n",
            "iter 4290: loss 1.2338, time 213.62ms, mfu 1.59%\n",
            "iter 4300: loss 1.2228, time 217.57ms, mfu 1.61%\n",
            "iter 4310: loss 1.2045, time 219.64ms, mfu 1.61%\n",
            "iter 4320: loss 1.2169, time 219.03ms, mfu 1.62%\n",
            "iter 4330: loss 1.2339, time 217.96ms, mfu 1.63%\n",
            "iter 4340: loss 1.2405, time 214.83ms, mfu 1.64%\n",
            "iter 4350: loss 1.2248, time 216.79ms, mfu 1.65%\n",
            "iter 4360: loss 1.2333, time 217.31ms, mfu 1.66%\n",
            "iter 4370: loss 1.2208, time 217.48ms, mfu 1.66%\n",
            "iter 4380: loss 1.1955, time 215.25ms, mfu 1.67%\n",
            "iter 4390: loss 1.2117, time 215.10ms, mfu 1.68%\n",
            "iter 4400: loss 1.2070, time 217.67ms, mfu 1.68%\n",
            "iter 4410: loss 1.2012, time 215.32ms, mfu 1.68%\n",
            "iter 4420: loss 1.2458, time 219.76ms, mfu 1.69%\n",
            "iter 4430: loss 1.2042, time 218.16ms, mfu 1.69%\n",
            "iter 4440: loss 1.1985, time 220.08ms, mfu 1.69%\n",
            "iter 4450: loss 1.2391, time 220.07ms, mfu 1.69%\n",
            "iter 4460: loss 1.2075, time 217.55ms, mfu 1.69%\n",
            "iter 4470: loss 1.2336, time 218.55ms, mfu 1.69%\n",
            "iter 4480: loss 1.2160, time 215.10ms, mfu 1.70%\n",
            "iter 4490: loss 1.2093, time 217.48ms, mfu 1.70%\n",
            "step 4500: train loss 1.1315, val loss 1.4703\n",
            "iter 4500: loss 1.2161, time 30691.62ms, mfu 1.53%\n",
            "iter 4510: loss 1.2189, time 218.97ms, mfu 1.55%\n",
            "iter 4520: loss 1.1987, time 215.51ms, mfu 1.56%\n",
            "iter 4530: loss 1.2195, time 216.88ms, mfu 1.58%\n",
            "iter 4540: loss 1.2153, time 218.23ms, mfu 1.59%\n",
            "iter 4550: loss 1.2401, time 214.64ms, mfu 1.61%\n",
            "iter 4560: loss 1.2437, time 217.97ms, mfu 1.62%\n",
            "iter 4570: loss 1.2253, time 217.25ms, mfu 1.63%\n",
            "iter 4580: loss 1.2311, time 219.83ms, mfu 1.63%\n",
            "iter 4590: loss 1.2433, time 218.74ms, mfu 1.64%\n",
            "iter 4600: loss 1.2098, time 216.62ms, mfu 1.65%\n",
            "iter 4610: loss 1.2340, time 215.48ms, mfu 1.66%\n",
            "iter 4620: loss 1.2112, time 216.41ms, mfu 1.66%\n",
            "iter 4630: loss 1.2228, time 215.48ms, mfu 1.67%\n",
            "iter 4640: loss 1.2297, time 220.94ms, mfu 1.67%\n",
            "iter 4650: loss 1.2318, time 219.23ms, mfu 1.67%\n",
            "iter 4660: loss 1.2052, time 217.82ms, mfu 1.68%\n",
            "iter 4670: loss 1.1947, time 216.04ms, mfu 1.68%\n",
            "iter 4680: loss 1.2456, time 218.04ms, mfu 1.69%\n",
            "iter 4690: loss 1.2329, time 220.28ms, mfu 1.69%\n",
            "iter 4700: loss 1.1880, time 215.03ms, mfu 1.69%\n",
            "iter 4710: loss 1.2076, time 216.64ms, mfu 1.69%\n",
            "iter 4720: loss 1.2228, time 219.44ms, mfu 1.69%\n",
            "iter 4730: loss 1.2194, time 218.63ms, mfu 1.70%\n",
            "iter 4740: loss 1.2179, time 217.37ms, mfu 1.70%\n",
            "step 4750: train loss 1.1291, val loss 1.4662\n",
            "iter 4750: loss 1.1786, time 30692.68ms, mfu 1.53%\n",
            "iter 4760: loss 1.2098, time 214.75ms, mfu 1.55%\n",
            "iter 4770: loss 1.2102, time 218.43ms, mfu 1.56%\n",
            "iter 4780: loss 1.2016, time 219.22ms, mfu 1.58%\n",
            "iter 4790: loss 1.2331, time 215.37ms, mfu 1.59%\n",
            "iter 4800: loss 1.2102, time 216.56ms, mfu 1.61%\n",
            "iter 4810: loss 1.2399, time 218.94ms, mfu 1.62%\n",
            "iter 4820: loss 1.2048, time 215.03ms, mfu 1.63%\n",
            "iter 4830: loss 1.2324, time 217.57ms, mfu 1.64%\n",
            "iter 4840: loss 1.2092, time 215.66ms, mfu 1.65%\n",
            "iter 4850: loss 1.2263, time 219.20ms, mfu 1.65%\n",
            "iter 4860: loss 1.2115, time 217.81ms, mfu 1.66%\n",
            "iter 4870: loss 1.2471, time 215.62ms, mfu 1.66%\n",
            "iter 4880: loss 1.2360, time 219.16ms, mfu 1.67%\n",
            "iter 4890: loss 1.2086, time 214.81ms, mfu 1.67%\n",
            "iter 4900: loss 1.2106, time 214.62ms, mfu 1.68%\n",
            "iter 4910: loss 1.2233, time 214.54ms, mfu 1.69%\n",
            "iter 4920: loss 1.2169, time 215.83ms, mfu 1.69%\n",
            "iter 4930: loss 1.2221, time 217.13ms, mfu 1.69%\n",
            "iter 4940: loss 1.2128, time 214.94ms, mfu 1.70%\n",
            "iter 4950: loss 1.2239, time 218.42ms, mfu 1.70%\n",
            "iter 4960: loss 1.1913, time 218.87ms, mfu 1.70%\n",
            "iter 4970: loss 1.2111, time 217.21ms, mfu 1.70%\n",
            "iter 4980: loss 1.2132, time 219.43ms, mfu 1.70%\n",
            "iter 4990: loss 1.1995, time 218.09ms, mfu 1.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "Run inference on the model. Complete the TODO portions\n",
        "\n",
        "1. You need to call `model.generate` in the given for loop.\n",
        "2. Show 10 samples. These might not be perfectly sensible English, but they should be very Shakespeare-like. Make sure they can be read in your submitted PDF."
      ],
      "metadata": {
        "id": "pLll_WMoma-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "model.eval()\n",
        "\n",
        "meta_path = './data/shakespeare/meta.pkl'\n",
        "print(f\"Loading meta from meta.pkl...\")\n",
        "with open(meta_path, 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta['stoi'], meta['itos']\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
      ],
      "metadata": {
        "id": "-BY0EmCMmRcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b43810-25ef-4be2-af09-93f959be9485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta from meta.pkl...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            gen = model.generate(x, max_new_tokens, temperature, top_k)\n",
        "            print(f\"Generated Example {k + 1}: \")\n",
        "            print(decode(gen[0].tolist()))\n",
        "            print()"
      ],
      "metadata": {
        "id": "nUHUenqJVkDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be987351-a1e7-4ae3-f3f7-39f58684683f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Example 0: \n",
            "\n",
            "DERBY:\n",
            "But now what you saw you?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Doth you here, not a tribunes of Capulet, who;\n",
            "Though to the battle content possessions,\n",
            "Nor that have no else to punish'd him at him;\n",
            "For thus thy give her divines discreature never\n",
            "Than thou shalt still answer a sight for princely here,\n",
            "And say the hath desires of thee and a loyal place.\n",
            "\n",
            "ABHORSON:\n",
            "I am so; even so sky and less\n",
            "That life be still to me tear it.\n",
            "\n",
            "ANGELO:\n",
            "My good lord: what, art thou believed?\n",
            "\n",
            "Third Citizen:\n",
            "Ay, like a side that\n",
            "\n",
            "Generated Example 1: \n",
            "\n",
            "She is a wife even black from my kind of king,\n",
            "Yet thou madest me almost desired man;\n",
            "For why, or I can blow them supply with the deed,\n",
            "Such success, as thou, shouldst thou broach out\n",
            "By an accusation of my case.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The gracious valour will not so break.\n",
            "\n",
            "LUCIO:\n",
            "No, no more?\n",
            "\n",
            "Provost:\n",
            "My lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "No, no, no, sir; I pray, Pompey your business.\n",
            "\n",
            "ISABELLA:\n",
            "Once would you be done.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Answer that 'tis made: but do you unmand, I see;\n",
            "And from your safe-hath yo\n",
            "\n",
            "Generated Example 2: \n",
            "\n",
            "No warrant with the better than the business, shall\n",
            "prevail up your own grief, and now they say they do\n",
            "to see the friends of the wind.\n",
            "\n",
            "Clown:\n",
            "Provost, sir, I pray myself.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Fear you, here's a good father; love, whence say he will please.\n",
            "\n",
            "Clown:\n",
            "We'll have you must you be so? an you were a general and\n",
            "shepherd-say it may be a vower of good worship, here\n",
            "be seen a man of a letterer.\n",
            "\n",
            "Shepherd:\n",
            "And I thought a very worse o' the silence of the\n",
            "wild.\n",
            "\n",
            "First Servingman:\n",
            "He hath born our f\n",
            "\n",
            "Generated Example 3: \n",
            "\n",
            "And into Mariana, if proclaime,\n",
            "His time is dead, and so remain\n",
            "Of the bosom of his royal tomb,\n",
            "And we would have not been aside, strike of such\n",
            "Her eyes passion of your king, I live, and throw\n",
            "Shows for my head by mine arms but the king,\n",
            "And yours unlikely steal on a hand.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Why do you should be so?\n",
            "\n",
            "RICHARD:\n",
            "Let us have plain to be our soul.\n",
            "\n",
            "RICHARD:\n",
            "Well, what you fair dares that perform'd is young?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "What say'st thou, Richard? what news?\n",
            "\n",
            "CLARENCE:\n",
            "No, good\n",
            "\n",
            "Generated Example 4: \n",
            "\n",
            "Put for him within the opinion, that\n",
            "has a father while she be married to virtue.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Your partner, sir, strike, I am a sister, a cracked turnes\n",
            "and wafter of disposition. The point of it\n",
            "may came to have my father sister, and your unburnings\n",
            "the enemy bear'd in the justice of her brother of\n",
            "the head of the gracious manner, why do you be marked me to be born\n",
            "that would have a done and let know you mine ears of mine own; you shall\n",
            "protest, indeed, and the vastage of him would have\n",
            "returne\n",
            "\n",
            "Generated Example 5: \n",
            "\n",
            "When they were they say with the duke and be man\n",
            "As they so in thy way the princes, which thou hast\n",
            "Then she hath rumed us to thy monstrots.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, that we say!\n",
            "\n",
            "MENENIUS:\n",
            "Women oxford, sir, if thou art thy delivers.\n",
            "You that be a thousand and state, at your service\n",
            "To curbs the sun the shadows of him.\n",
            "\n",
            "MENENIUS:\n",
            "The people's of your watch friends: look you we\n",
            "Are warn, as he will scarried up.\n",
            "\n",
            "ESCALUS:\n",
            "My lord, my lord, I shall tell.\n",
            "\n",
            "COMINIUS:\n",
            "He was your well: but yet, then, I will \n",
            "\n",
            "Generated Example 6: \n",
            "\n",
            "\n",
            "SICINIUS:\n",
            "He knows not by them.\n",
            "\n",
            "SICINIUS:\n",
            "He was a metter house with the\n",
            "city that command his desire. But what a tower?\n",
            "\n",
            "Shepherd:\n",
            "Master, your news?\n",
            "\n",
            "First Senator:\n",
            "Come, sir, sir,\n",
            "You should be worn.\n",
            "\n",
            "CORIOLANUS:\n",
            "No more.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, I will go by the rough and man.\n",
            "\n",
            "MENENIUS:\n",
            "There lives should see how 'twas rulely as\n",
            "this bawd, that shows strange him were cold me: he was the\n",
            "fench, and our veins are till do the whole could were\n",
            "we they can caution, for thence they have seem a banished\n",
            "\n",
            "Generated Example 7: \n",
            "\n",
            "Marry you, Marcius, and thence there, you are\n",
            "putter them.\n",
            "\n",
            "COMINIUS:\n",
            "And, sir, sir! Where is the greater feasting to great speak, or he\n",
            "will be speak with steel, there indeed grossips with one\n",
            "the city: therefore we have more heardonest sleepimalice\n",
            "with Rome, and break content to the rock\n",
            "To the common of his worse, to provoke a ceaparer\n",
            "And her aid the purpose.\n",
            "\n",
            "MENENIUS:\n",
            "What's the price of the steel?\n",
            "\n",
            "Second Citizen:\n",
            "You are at redempted by the scope hours\n",
            "of thy desirestruction, how in the\n",
            "\n",
            "Generated Example 8: \n",
            "\n",
            "And that's to be protector'd in love\n",
            "Until the encounterproy'd, and hope itself,\n",
            "I mean, and then we will prove my kneel will send\n",
            "By Lancasters of the assemble and mine.\n",
            "\n",
            "WARWICK:\n",
            "Go, by the good Christian Buckingham,\n",
            "And hast thou shouldst seek off my warlike strong?\n",
            "Nor the man is present and conduct of your honour;\n",
            "And myself will find the best part of his crown,\n",
            "Nor more command's cause to reach in presence:\n",
            "Which Henry, to this is the fiury war refuses\n",
            "Like my father's father's prince; whe\n",
            "\n",
            "Generated Example 9: \n",
            "\n",
            "\n",
            "All thou that mayst know the time of his death?\n",
            "\n",
            "First Murderer:\n",
            "O my tress will unborn my body.\n",
            "\n",
            "First Murderer:\n",
            "Well, I fear thee; what near to be come.\n",
            "\n",
            "MENENIUS:\n",
            "How now! what! which state a pleasure?\n",
            "\n",
            "First Citizen:\n",
            "An how my can tell in his happy mistress farewell, but as\n",
            "the common of all that do this mistress.\n",
            "\n",
            "Second Citizen:\n",
            "I am a more than the trade haunts before a people?\n",
            "\n",
            "SICINIUS:\n",
            "He thinks the noble people. Beseech you for your heart\n",
            "honours he did the people vanity of\n",
            "the senat\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5: Train the model on a new dataset and show results.\n",
        "This exercise is mostly about making sure you can find and preprocess text, as well as checking that you understand the above code well enough to reuse it.\n",
        "1. Find some text data. Use our Shakespeare file as reference. You will want a similar amount of text data. Don't go overboard- a big text file will just make things take too long.\n",
        "2. Perform any preprocessing necessary to get the text ready for the model. Use the preprocessing code we provide as reference.\n",
        "3. Train the model on your text.\n",
        "4. Generate and print 10 samples from the model trained on your text.\n",
        "\n",
        "You may want to implement the functions below, using the code in the previous cells. Or not! It's up to you. You just need to write code that can train a model to generate text from some non-Shakespeare data. The generated text is the main deliverable that most of the grade will be based on. Make sure it displays prominently in your submitted PDF."
      ],
      "metadata": {
        "id": "4qvKninXYpdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data():\n",
        "  \"\"\"This should download your data and format it according to your need for training and inference.\n",
        "  This function should also print the statistics on the data like vocab_size, length of dataset, etc.\"\"\"\n",
        "  # download the tiny shakespeare dataset\n",
        "  if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "  if not os.path.exists('data/trump'):\n",
        "    os.makedirs('data/trump')\n",
        "  data_root = 'data/trump'\n",
        "  input_file_path = os.path.join(data_root, 'input.txt')\n",
        "  if not os.path.exists(input_file_path):\n",
        "      data_url = 'https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt'\n",
        "      with open(input_file_path, 'w') as f:\n",
        "          f.write(requests.get(data_url).text)\n",
        "\n",
        "  with open(input_file_path, 'r') as f:\n",
        "      data = f.read()\n",
        "  print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "  # get all the unique characters that occur in this text\n",
        "  chars = sorted(list(set(data)))\n",
        "  vocab_size = len(chars)\n",
        "  print(\"all the unique characters:\", ''.join(chars))\n",
        "  print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "  # create a mapping from characters to integers\n",
        "  stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "  itos = { i:ch for i,ch in enumerate(chars) }\n",
        "  def encode(s):\n",
        "      return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "  def decode(l):\n",
        "      return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "  # create the train and test splits\n",
        "  n = len(data)\n",
        "  train_data = data[:int(n*0.9)]\n",
        "  val_data = data[int(n*0.9):]\n",
        "\n",
        "  # encode both to integers\n",
        "  train_ids = encode(train_data)\n",
        "  val_ids = encode(val_data)\n",
        "  print(f\"train has {len(train_ids):,} tokens\")\n",
        "  print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "  # export to bin files\n",
        "  train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "  val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "  train_ids.tofile(os.path.join(data_root, 'train.bin'))\n",
        "  val_ids.tofile(os.path.join(data_root, 'val.bin'))\n",
        "\n",
        "  # save the meta information as well, to help us encode/decode later\n",
        "  meta = {\n",
        "      'vocab_size': vocab_size,\n",
        "      'itos': itos,\n",
        "      'stoi': stoi,\n",
        "  }\n",
        "  with open(f'{data_root}/meta.pkl', 'wb') as f:\n",
        "      pickle.dump(meta, f)\n",
        "\n",
        "def train_model():\n",
        "  \"\"\"Train the model that is defined in Exercise 1 on your train data\"\"\"\n",
        "  ## TRAIN CONFIG\n",
        "  out_dir = 'out-trump-char'\n",
        "  eval_interval = 250\n",
        "  log_interval = 10\n",
        "  eval_iters = 200\n",
        "  eval_only = False\n",
        "  always_save_checkpoint = False\n",
        "  # data\n",
        "  dataset = 'trump'\n",
        "  gradient_accumulation_steps = 1\n",
        "  batch_size = 64\n",
        "  block_size = 256\n",
        "  # model\n",
        "  n_layer = 6\n",
        "  n_head = 6\n",
        "  n_embd = 384\n",
        "  dropout = 0.2\n",
        "  bias =  False\n",
        "  # adamw optimizer\n",
        "  learning_rate = 1e-3\n",
        "  max_iters = 5000 # 5000\n",
        "  weight_decay = 1e-1\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.99\n",
        "  grad_clip = 1.0\n",
        "  decay_lr = True\n",
        "  warmup_iters = 100\n",
        "  lr_decay_iters = 5000 # 5000\n",
        "  min_lr = 1e-4\n",
        "  # system\n",
        "  device = 'cuda'\n",
        "  dtype = 'float16'\n",
        "  compile = False\n",
        "\n",
        "  seed_offset = 0\n",
        "  ddp_world_size = 1\n",
        "  tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "  print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "  torch.manual_seed(1337 + seed_offset)\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "  ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n",
        "  ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "  data_dir = os.path.join('data', dataset)\n",
        "  train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "  val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "  def get_batch(split):\n",
        "      data = train_data if split == 'train' else val_data\n",
        "      ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "      x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "      y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "      if device_type == 'cuda':\n",
        "          x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "      else:\n",
        "          x, y = x.to(device), y.to(device)\n",
        "      return x, y\n",
        "\n",
        "  iter_num = 0\n",
        "  best_val_loss = 1e9\n",
        "\n",
        "  meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "  meta_vocab_size = None\n",
        "  if os.path.exists(meta_path):\n",
        "      with open(meta_path, 'rb') as f:\n",
        "          meta = pickle.load(f)\n",
        "      meta_vocab_size = meta['vocab_size']\n",
        "      print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "  # model init\n",
        "  model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                    bias=bias, vocab_size=None, dropout=dropout)\n",
        "  model_args['vocab_size'] = meta_vocab_size\n",
        "  gptconf = GPTConfig(**model_args)\n",
        "  model = GPT(gptconf)\n",
        "  if block_size < model.config.block_size:\n",
        "      model.crop_block_size(block_size)\n",
        "      model_args['block_size'] = block_size\n",
        "  model.to(device)\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "  checkpoint = None\n",
        "\n",
        "  # compile the model\n",
        "  if compile:\n",
        "      print(\"compiling the model... (takes a ~minute)\")\n",
        "      unoptimized_model = model\n",
        "      model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss():\n",
        "      out = {}\n",
        "      model.eval()\n",
        "      for split in ['train', 'val']:\n",
        "          losses = torch.zeros(eval_iters)\n",
        "          for k in range(eval_iters):\n",
        "              X, Y = get_batch(split)\n",
        "              with ctx:\n",
        "                  logits, loss = model(X, Y)\n",
        "              losses[k] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "      model.train()\n",
        "      return out\n",
        "\n",
        "  # training loop\n",
        "  X, Y = get_batch('train') # fetch the very first batch\n",
        "  t0 = time.time()\n",
        "  local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "  raw_model = model\n",
        "  running_mfu = -1.0\n",
        "  for iter_num in range(max_iters):\n",
        "      lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = lr\n",
        "\n",
        "      if iter_num % eval_interval == 0:\n",
        "          losses = estimate_loss()\n",
        "          print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "          if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "              best_val_loss = losses['val']\n",
        "              if iter_num > 0:\n",
        "                  checkpoint = {\n",
        "                      'model': raw_model.state_dict(),\n",
        "                      'optimizer': optimizer.state_dict(),\n",
        "                      'model_args': model_args,\n",
        "                      'iter_num': iter_num,\n",
        "                      'best_val_loss': best_val_loss\n",
        "                  }\n",
        "                  print(f\"saving checkpoint to {out_dir}\")\n",
        "                  torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "      if iter_num == 0 and eval_only:\n",
        "          break\n",
        "\n",
        "      for micro_step in range(gradient_accumulation_steps):\n",
        "          with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "              loss = loss / gradient_accumulation_steps\n",
        "          X, Y = get_batch('train')\n",
        "          scaler.scale(loss).backward()\n",
        "      if grad_clip != 0.0:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "      if iter_num % log_interval == 0:\n",
        "          lossf = loss.item() * gradient_accumulation_steps\n",
        "          if local_iter_num >= 5: # let the training loop settle a bit\n",
        "              mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "              running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "          print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "      local_iter_num += 1\n",
        "  return model\n",
        "\n",
        "\n",
        "def eval_model(model):\n",
        "  \"\"\"Runs inference of the trained model on your test data\"\"\"\n",
        "  # -----------------------------------------------------------------------------\n",
        "  start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "  num_samples = 10 # number of samples to draw\n",
        "  max_new_tokens = 500 # number of tokens generated in each sample\n",
        "  temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "  top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "  # -----------------------------------------------------------------------------\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  meta_path = './data/trump/meta.pkl'\n",
        "  print(f\"Loading meta from meta.pkl...\")\n",
        "  with open(meta_path, 'rb') as f:\n",
        "      meta = pickle.load(f)\n",
        "  stoi, itos = meta['stoi'], meta['itos']\n",
        "  encode = lambda s: [stoi[c] for c in s]\n",
        "  decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "  # encode the beginning of the prompt\n",
        "  if start.startswith('FILE:'):\n",
        "      with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "          start = f.read()\n",
        "  start_ids = encode(start)\n",
        "  x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "  # run generation\n",
        "  with torch.no_grad():\n",
        "      with ctx:\n",
        "          for k in range(num_samples):\n",
        "              gen = model.generate(x, max_new_tokens, temperature, top_k)\n",
        "              print(f\"Generated Example {k + 1}: \")\n",
        "              print(decode(gen[0].tolist()))\n",
        "              print()"
      ],
      "metadata": {
        "id": "WqEIp3WOnE_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1dCWi_686JM",
        "outputId": "b6b091b5-4ba5-4576-f155-a201268b6c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 896,270\n",
            "all the unique characters: \n",
            " !\"$%&'(),-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 93\n",
            "train has 806,643 tokens\n",
            "val has 89,627 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trump_model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4X91X_M8-be",
        "outputId": "17f318cc-637b-4b92-9e21-5505c3c2208b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 93 (inside data/trump/meta.pkl)\n",
            "number of parameters: 10.66M\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.6974, val loss 4.7359\n",
            "iter 0: loss 4.6761, time 30049.88ms, mfu -100.00%\n",
            "iter 10: loss 3.4747, time 211.02ms, mfu 1.77%\n",
            "iter 20: loss 3.0899, time 211.91ms, mfu 1.77%\n",
            "iter 30: loss 2.7529, time 210.29ms, mfu 1.77%\n",
            "iter 40: loss 2.5879, time 212.13ms, mfu 1.77%\n",
            "iter 50: loss 2.5188, time 212.75ms, mfu 1.77%\n",
            "iter 60: loss 2.4774, time 211.70ms, mfu 1.76%\n",
            "iter 70: loss 2.4519, time 212.64ms, mfu 1.76%\n",
            "iter 80: loss 2.4479, time 211.74ms, mfu 1.76%\n",
            "iter 90: loss 2.4241, time 210.84ms, mfu 1.76%\n",
            "iter 100: loss 2.4156, time 211.72ms, mfu 1.76%\n",
            "iter 110: loss 2.4037, time 210.30ms, mfu 1.76%\n",
            "iter 120: loss 2.4052, time 210.87ms, mfu 1.77%\n",
            "iter 130: loss 2.4246, time 213.00ms, mfu 1.76%\n",
            "iter 140: loss 2.3858, time 211.21ms, mfu 1.76%\n",
            "iter 150: loss 2.3808, time 211.61ms, mfu 1.76%\n",
            "iter 160: loss 2.3632, time 212.03ms, mfu 1.76%\n",
            "iter 170: loss 2.3610, time 213.02ms, mfu 1.76%\n",
            "iter 180: loss 2.3870, time 213.32ms, mfu 1.76%\n",
            "iter 190: loss 2.3687, time 212.12ms, mfu 1.76%\n",
            "iter 200: loss 2.3870, time 211.67ms, mfu 1.76%\n",
            "iter 210: loss 2.3553, time 211.78ms, mfu 1.76%\n",
            "iter 220: loss 2.3427, time 212.46ms, mfu 1.76%\n",
            "iter 230: loss 2.3575, time 212.03ms, mfu 1.76%\n",
            "iter 240: loss 2.3499, time 212.25ms, mfu 1.76%\n",
            "step 250: train loss 2.3221, val loss 3.9021\n",
            "saving checkpoint to out-trump-char\n",
            "iter 250: loss 2.3580, time 30566.10ms, mfu 1.58%\n",
            "iter 260: loss 2.3131, time 214.63ms, mfu 1.60%\n",
            "iter 270: loss 2.3304, time 214.82ms, mfu 1.61%\n",
            "iter 280: loss 2.3014, time 214.30ms, mfu 1.63%\n",
            "iter 290: loss 2.3156, time 214.23ms, mfu 1.64%\n",
            "iter 300: loss 2.2768, time 212.66ms, mfu 1.65%\n",
            "iter 310: loss 2.2912, time 214.23ms, mfu 1.66%\n",
            "iter 320: loss 2.2558, time 215.50ms, mfu 1.67%\n",
            "iter 330: loss 2.2209, time 214.39ms, mfu 1.67%\n",
            "iter 340: loss 2.2336, time 213.54ms, mfu 1.68%\n",
            "iter 350: loss 2.1615, time 217.50ms, mfu 1.68%\n",
            "iter 360: loss 2.1648, time 215.02ms, mfu 1.69%\n",
            "iter 370: loss 2.1581, time 216.17ms, mfu 1.69%\n",
            "iter 380: loss 2.1469, time 213.95ms, mfu 1.70%\n",
            "iter 390: loss 2.1156, time 216.28ms, mfu 1.70%\n",
            "iter 400: loss 2.0820, time 215.51ms, mfu 1.70%\n",
            "iter 410: loss 2.0168, time 215.10ms, mfu 1.71%\n",
            "iter 420: loss 2.0170, time 213.49ms, mfu 1.71%\n",
            "iter 430: loss 2.0355, time 213.43ms, mfu 1.71%\n",
            "iter 440: loss 1.9866, time 213.85ms, mfu 1.72%\n",
            "iter 450: loss 1.9642, time 215.48ms, mfu 1.72%\n",
            "iter 460: loss 1.9630, time 216.64ms, mfu 1.72%\n",
            "iter 470: loss 1.9431, time 214.56ms, mfu 1.72%\n",
            "iter 480: loss 1.9268, time 215.13ms, mfu 1.72%\n",
            "iter 490: loss 1.8868, time 215.10ms, mfu 1.72%\n",
            "step 500: train loss 1.7197, val loss 2.8722\n",
            "saving checkpoint to out-trump-char\n",
            "iter 500: loss 1.8466, time 30765.96ms, mfu 1.55%\n",
            "iter 510: loss 1.7947, time 216.39ms, mfu 1.57%\n",
            "iter 520: loss 1.7632, time 222.41ms, mfu 1.58%\n",
            "iter 530: loss 1.7792, time 215.78ms, mfu 1.59%\n",
            "iter 540: loss 1.7659, time 216.44ms, mfu 1.61%\n",
            "iter 550: loss 1.7477, time 218.74ms, mfu 1.62%\n",
            "iter 560: loss 1.7188, time 217.48ms, mfu 1.63%\n",
            "iter 570: loss 1.7073, time 214.00ms, mfu 1.64%\n",
            "iter 580: loss 1.7174, time 213.75ms, mfu 1.65%\n",
            "iter 590: loss 1.6729, time 213.33ms, mfu 1.66%\n",
            "iter 600: loss 1.6943, time 213.41ms, mfu 1.67%\n",
            "iter 610: loss 1.7082, time 214.12ms, mfu 1.68%\n",
            "iter 620: loss 1.6426, time 215.12ms, mfu 1.68%\n",
            "iter 630: loss 1.6409, time 216.98ms, mfu 1.69%\n",
            "iter 640: loss 1.6333, time 217.66ms, mfu 1.69%\n",
            "iter 650: loss 1.6389, time 220.13ms, mfu 1.69%\n",
            "iter 660: loss 1.6583, time 217.12ms, mfu 1.69%\n",
            "iter 670: loss 1.6452, time 213.47ms, mfu 1.70%\n",
            "iter 680: loss 1.5782, time 214.99ms, mfu 1.70%\n",
            "iter 690: loss 1.5776, time 211.72ms, mfu 1.71%\n",
            "iter 700: loss 1.5980, time 219.27ms, mfu 1.71%\n",
            "iter 710: loss 1.6024, time 215.64ms, mfu 1.71%\n",
            "iter 720: loss 1.5907, time 217.48ms, mfu 1.71%\n",
            "iter 730: loss 1.5480, time 214.72ms, mfu 1.71%\n",
            "iter 740: loss 1.5597, time 214.74ms, mfu 1.71%\n",
            "step 750: train loss 1.4136, val loss 2.5575\n",
            "saving checkpoint to out-trump-char\n",
            "iter 750: loss 1.5341, time 31283.31ms, mfu 1.54%\n",
            "iter 760: loss 1.5173, time 214.55ms, mfu 1.56%\n",
            "iter 770: loss 1.4715, time 218.99ms, mfu 1.58%\n",
            "iter 780: loss 1.4972, time 220.93ms, mfu 1.59%\n",
            "iter 790: loss 1.5046, time 218.82ms, mfu 1.60%\n",
            "iter 800: loss 1.4769, time 219.08ms, mfu 1.61%\n",
            "iter 810: loss 1.4759, time 215.49ms, mfu 1.62%\n",
            "iter 820: loss 1.4997, time 216.66ms, mfu 1.63%\n",
            "iter 830: loss 1.4537, time 216.17ms, mfu 1.64%\n",
            "iter 840: loss 1.5183, time 218.20ms, mfu 1.65%\n",
            "iter 850: loss 1.4960, time 219.91ms, mfu 1.65%\n",
            "iter 860: loss 1.4341, time 216.55ms, mfu 1.66%\n",
            "iter 870: loss 1.4446, time 220.89ms, mfu 1.66%\n",
            "iter 880: loss 1.4726, time 218.39ms, mfu 1.67%\n",
            "iter 890: loss 1.4257, time 234.47ms, mfu 1.66%\n",
            "iter 900: loss 1.4061, time 218.93ms, mfu 1.66%\n",
            "iter 910: loss 1.4741, time 218.26ms, mfu 1.67%\n",
            "iter 920: loss 1.4075, time 215.79ms, mfu 1.67%\n",
            "iter 930: loss 1.4293, time 219.08ms, mfu 1.68%\n",
            "iter 940: loss 1.4050, time 220.06ms, mfu 1.68%\n",
            "iter 950: loss 1.4030, time 218.30ms, mfu 1.68%\n",
            "iter 960: loss 1.4222, time 214.55ms, mfu 1.69%\n",
            "iter 970: loss 1.3981, time 214.95ms, mfu 1.69%\n",
            "iter 980: loss 1.4371, time 215.18ms, mfu 1.70%\n",
            "iter 990: loss 1.3519, time 214.27ms, mfu 1.70%\n",
            "step 1000: train loss 1.2659, val loss 2.5098\n",
            "saving checkpoint to out-trump-char\n",
            "iter 1000: loss 1.4022, time 31213.75ms, mfu 1.53%\n",
            "iter 1010: loss 1.3643, time 218.60ms, mfu 1.55%\n",
            "iter 1020: loss 1.3797, time 218.97ms, mfu 1.56%\n",
            "iter 1030: loss 1.4002, time 217.15ms, mfu 1.58%\n",
            "iter 1040: loss 1.3375, time 215.40ms, mfu 1.60%\n",
            "iter 1050: loss 1.3790, time 216.48ms, mfu 1.61%\n",
            "iter 1060: loss 1.3648, time 217.06ms, mfu 1.62%\n",
            "iter 1070: loss 1.3725, time 217.38ms, mfu 1.63%\n",
            "iter 1080: loss 1.3562, time 218.63ms, mfu 1.64%\n",
            "iter 1090: loss 1.3445, time 225.70ms, mfu 1.64%\n",
            "iter 1100: loss 1.3130, time 219.81ms, mfu 1.64%\n",
            "iter 1110: loss 1.3553, time 218.70ms, mfu 1.65%\n",
            "iter 1120: loss 1.3349, time 214.49ms, mfu 1.66%\n",
            "iter 1130: loss 1.3379, time 215.57ms, mfu 1.67%\n",
            "iter 1140: loss 1.3607, time 215.95ms, mfu 1.67%\n",
            "iter 1150: loss 1.3282, time 217.65ms, mfu 1.68%\n",
            "iter 1160: loss 1.2808, time 216.61ms, mfu 1.68%\n",
            "iter 1170: loss 1.3281, time 214.34ms, mfu 1.69%\n",
            "iter 1180: loss 1.2480, time 215.98ms, mfu 1.69%\n",
            "iter 1190: loss 1.3383, time 219.98ms, mfu 1.69%\n",
            "iter 1200: loss 1.3261, time 217.05ms, mfu 1.69%\n",
            "iter 1210: loss 1.3190, time 218.85ms, mfu 1.69%\n",
            "iter 1220: loss 1.3215, time 215.46ms, mfu 1.70%\n",
            "iter 1230: loss 1.2826, time 218.45ms, mfu 1.70%\n",
            "iter 1240: loss 1.3314, time 218.14ms, mfu 1.70%\n",
            "step 1250: train loss 1.1786, val loss 2.4391\n",
            "saving checkpoint to out-trump-char\n",
            "iter 1250: loss 1.2803, time 30960.27ms, mfu 1.53%\n",
            "iter 1260: loss 1.2889, time 216.93ms, mfu 1.55%\n",
            "iter 1270: loss 1.2770, time 217.38ms, mfu 1.57%\n",
            "iter 1280: loss 1.2643, time 218.71ms, mfu 1.58%\n",
            "iter 1290: loss 1.2875, time 215.37ms, mfu 1.60%\n",
            "iter 1300: loss 1.2822, time 216.69ms, mfu 1.61%\n",
            "iter 1310: loss 1.2909, time 210.92ms, mfu 1.62%\n",
            "iter 1320: loss 1.2960, time 216.95ms, mfu 1.63%\n",
            "iter 1330: loss 1.2588, time 216.93ms, mfu 1.64%\n",
            "iter 1340: loss 1.2546, time 214.64ms, mfu 1.65%\n",
            "iter 1350: loss 1.2960, time 218.32ms, mfu 1.66%\n",
            "iter 1360: loss 1.2735, time 217.38ms, mfu 1.66%\n",
            "iter 1370: loss 1.2652, time 215.02ms, mfu 1.67%\n",
            "iter 1380: loss 1.2510, time 216.04ms, mfu 1.68%\n",
            "iter 1390: loss 1.2769, time 217.70ms, mfu 1.68%\n",
            "iter 1400: loss 1.2548, time 219.80ms, mfu 1.68%\n",
            "iter 1410: loss 1.2333, time 217.23ms, mfu 1.69%\n",
            "iter 1420: loss 1.2549, time 217.62ms, mfu 1.69%\n",
            "iter 1430: loss 1.2511, time 219.10ms, mfu 1.69%\n",
            "iter 1440: loss 1.2618, time 217.39ms, mfu 1.69%\n",
            "iter 1450: loss 1.2446, time 215.31ms, mfu 1.70%\n",
            "iter 1460: loss 1.2416, time 216.05ms, mfu 1.70%\n",
            "iter 1470: loss 1.2136, time 219.31ms, mfu 1.70%\n",
            "iter 1480: loss 1.2543, time 215.25ms, mfu 1.70%\n",
            "iter 1490: loss 1.2437, time 216.48ms, mfu 1.70%\n",
            "step 1500: train loss 1.1088, val loss 2.4175\n",
            "saving checkpoint to out-trump-char\n",
            "iter 1500: loss 1.2520, time 31036.74ms, mfu 1.54%\n",
            "iter 1510: loss 1.2470, time 217.46ms, mfu 1.55%\n",
            "iter 1520: loss 1.2600, time 215.67ms, mfu 1.57%\n",
            "iter 1530: loss 1.2050, time 217.89ms, mfu 1.58%\n",
            "iter 1540: loss 1.2193, time 215.11ms, mfu 1.60%\n",
            "iter 1550: loss 1.2093, time 217.93ms, mfu 1.61%\n",
            "iter 1560: loss 1.2055, time 218.44ms, mfu 1.62%\n",
            "iter 1570: loss 1.2178, time 216.85ms, mfu 1.63%\n",
            "iter 1580: loss 1.1964, time 218.98ms, mfu 1.64%\n",
            "iter 1590: loss 1.2177, time 218.35ms, mfu 1.64%\n",
            "iter 1600: loss 1.2130, time 215.92ms, mfu 1.65%\n",
            "iter 1610: loss 1.2000, time 215.74ms, mfu 1.66%\n",
            "iter 1620: loss 1.1745, time 219.58ms, mfu 1.66%\n",
            "iter 1630: loss 1.2075, time 216.75ms, mfu 1.67%\n",
            "iter 1640: loss 1.1813, time 219.06ms, mfu 1.67%\n",
            "iter 1650: loss 1.1842, time 217.01ms, mfu 1.68%\n",
            "iter 1660: loss 1.2060, time 219.72ms, mfu 1.68%\n",
            "iter 1670: loss 1.1969, time 218.46ms, mfu 1.68%\n",
            "iter 1680: loss 1.1807, time 217.24ms, mfu 1.69%\n",
            "iter 1690: loss 1.1766, time 215.01ms, mfu 1.69%\n",
            "iter 1700: loss 1.1789, time 219.00ms, mfu 1.69%\n",
            "iter 1710: loss 1.1700, time 217.44ms, mfu 1.69%\n",
            "iter 1720: loss 1.1942, time 218.10ms, mfu 1.70%\n",
            "iter 1730: loss 1.1825, time 216.50ms, mfu 1.70%\n",
            "iter 1740: loss 1.1728, time 220.00ms, mfu 1.70%\n",
            "step 1750: train loss 1.0566, val loss 2.3745\n",
            "saving checkpoint to out-trump-char\n",
            "iter 1750: loss 1.1798, time 31017.00ms, mfu 1.53%\n",
            "iter 1760: loss 1.1904, time 218.93ms, mfu 1.55%\n",
            "iter 1770: loss 1.1791, time 215.27ms, mfu 1.57%\n",
            "iter 1780: loss 1.2148, time 217.96ms, mfu 1.58%\n",
            "iter 1790: loss 1.1761, time 218.89ms, mfu 1.59%\n",
            "iter 1800: loss 1.1617, time 217.15ms, mfu 1.61%\n",
            "iter 1810: loss 1.1283, time 215.30ms, mfu 1.62%\n",
            "iter 1820: loss 1.1638, time 219.36ms, mfu 1.63%\n",
            "iter 1830: loss 1.1485, time 216.69ms, mfu 1.64%\n",
            "iter 1840: loss 1.1172, time 219.85ms, mfu 1.64%\n",
            "iter 1850: loss 1.1181, time 217.74ms, mfu 1.65%\n",
            "iter 1860: loss 1.1254, time 220.61ms, mfu 1.65%\n",
            "iter 1870: loss 1.1743, time 215.68ms, mfu 1.66%\n",
            "iter 1880: loss 1.1637, time 216.63ms, mfu 1.67%\n",
            "iter 1890: loss 1.1528, time 217.84ms, mfu 1.67%\n",
            "iter 1900: loss 1.1436, time 218.04ms, mfu 1.68%\n",
            "iter 1910: loss 1.1450, time 219.15ms, mfu 1.68%\n",
            "iter 1920: loss 1.1719, time 216.70ms, mfu 1.68%\n",
            "iter 1930: loss 1.1315, time 217.46ms, mfu 1.69%\n",
            "iter 1940: loss 1.1366, time 214.01ms, mfu 1.69%\n",
            "iter 1950: loss 1.1403, time 219.48ms, mfu 1.69%\n",
            "iter 1960: loss 1.1296, time 220.45ms, mfu 1.69%\n",
            "iter 1970: loss 1.1266, time 220.62ms, mfu 1.69%\n",
            "iter 1980: loss 1.1097, time 215.71ms, mfu 1.70%\n",
            "iter 1990: loss 1.1141, time 217.76ms, mfu 1.70%\n",
            "step 2000: train loss 1.0169, val loss 2.3606\n",
            "saving checkpoint to out-trump-char\n",
            "iter 2000: loss 1.1054, time 31089.20ms, mfu 1.53%\n",
            "iter 2010: loss 1.1240, time 214.46ms, mfu 1.55%\n",
            "iter 2020: loss 1.1162, time 218.73ms, mfu 1.57%\n",
            "iter 2030: loss 1.1312, time 214.77ms, mfu 1.58%\n",
            "iter 2040: loss 1.1229, time 218.44ms, mfu 1.59%\n",
            "iter 2050: loss 1.1379, time 217.88ms, mfu 1.61%\n",
            "iter 2060: loss 1.0858, time 219.00ms, mfu 1.62%\n",
            "iter 2070: loss 1.1108, time 217.24ms, mfu 1.63%\n",
            "iter 2080: loss 1.1203, time 219.66ms, mfu 1.63%\n",
            "iter 2090: loss 1.1154, time 220.21ms, mfu 1.64%\n",
            "iter 2100: loss 1.0878, time 216.28ms, mfu 1.65%\n",
            "iter 2110: loss 1.0988, time 219.65ms, mfu 1.65%\n",
            "iter 2120: loss 1.0937, time 217.80ms, mfu 1.66%\n",
            "iter 2130: loss 1.0994, time 219.17ms, mfu 1.66%\n",
            "iter 2140: loss 1.0857, time 218.73ms, mfu 1.67%\n",
            "iter 2150: loss 1.0730, time 217.21ms, mfu 1.67%\n",
            "iter 2160: loss 1.1056, time 214.58ms, mfu 1.68%\n",
            "iter 2170: loss 1.0650, time 219.59ms, mfu 1.68%\n",
            "iter 2180: loss 1.0988, time 218.23ms, mfu 1.68%\n",
            "iter 2190: loss 1.1035, time 219.46ms, mfu 1.69%\n",
            "iter 2200: loss 1.0659, time 216.49ms, mfu 1.69%\n",
            "iter 2210: loss 1.0792, time 219.77ms, mfu 1.69%\n",
            "iter 2220: loss 1.0661, time 217.08ms, mfu 1.69%\n",
            "iter 2230: loss 1.0652, time 219.27ms, mfu 1.69%\n",
            "iter 2240: loss 1.0668, time 216.07ms, mfu 1.70%\n",
            "step 2250: train loss 0.9761, val loss 2.4061\n",
            "iter 2250: loss 1.1023, time 30715.57ms, mfu 1.53%\n",
            "iter 2260: loss 1.0822, time 218.51ms, mfu 1.55%\n",
            "iter 2270: loss 1.1341, time 219.41ms, mfu 1.56%\n",
            "iter 2280: loss 1.1322, time 219.02ms, mfu 1.58%\n",
            "iter 2290: loss 1.0909, time 219.50ms, mfu 1.59%\n",
            "iter 2300: loss 1.1042, time 219.21ms, mfu 1.60%\n",
            "iter 2310: loss 1.0715, time 218.90ms, mfu 1.61%\n",
            "iter 2320: loss 1.1015, time 217.35ms, mfu 1.62%\n",
            "iter 2330: loss 1.0863, time 218.49ms, mfu 1.63%\n",
            "iter 2340: loss 1.0826, time 217.08ms, mfu 1.64%\n",
            "iter 2350: loss 1.0832, time 219.52ms, mfu 1.64%\n",
            "iter 2360: loss 1.0749, time 217.95ms, mfu 1.65%\n",
            "iter 2370: loss 1.0601, time 217.93ms, mfu 1.66%\n",
            "iter 2380: loss 1.0970, time 217.24ms, mfu 1.66%\n",
            "iter 2390: loss 1.0856, time 215.13ms, mfu 1.67%\n",
            "iter 2400: loss 1.0596, time 219.11ms, mfu 1.67%\n",
            "iter 2410: loss 1.1282, time 215.67ms, mfu 1.68%\n",
            "iter 2420: loss 1.1054, time 218.91ms, mfu 1.68%\n",
            "iter 2430: loss 1.0943, time 217.60ms, mfu 1.68%\n",
            "iter 2440: loss 1.0775, time 218.95ms, mfu 1.69%\n",
            "iter 2450: loss 1.0879, time 215.64ms, mfu 1.69%\n",
            "iter 2460: loss 1.0647, time 214.62ms, mfu 1.70%\n",
            "iter 2470: loss 1.0366, time 217.24ms, mfu 1.70%\n",
            "iter 2480: loss 1.0736, time 218.02ms, mfu 1.70%\n",
            "iter 2490: loss 1.0464, time 214.90ms, mfu 1.70%\n",
            "step 2500: train loss 0.9533, val loss 2.4159\n",
            "iter 2500: loss 1.0776, time 30751.73ms, mfu 1.53%\n",
            "iter 2510: loss 1.0315, time 219.77ms, mfu 1.55%\n",
            "iter 2520: loss 1.0572, time 215.71ms, mfu 1.57%\n",
            "iter 2530: loss 1.0546, time 215.52ms, mfu 1.58%\n",
            "iter 2540: loss 1.0580, time 218.58ms, mfu 1.60%\n",
            "iter 2550: loss 1.0786, time 216.83ms, mfu 1.61%\n",
            "iter 2560: loss 1.0388, time 217.03ms, mfu 1.62%\n",
            "iter 2570: loss 1.0846, time 216.56ms, mfu 1.63%\n",
            "iter 2580: loss 1.0488, time 218.78ms, mfu 1.64%\n",
            "iter 2590: loss 1.0820, time 218.07ms, mfu 1.64%\n",
            "iter 2600: loss 1.0293, time 219.45ms, mfu 1.65%\n",
            "iter 2610: loss 1.0227, time 218.39ms, mfu 1.66%\n",
            "iter 2620: loss 1.0612, time 220.30ms, mfu 1.66%\n",
            "iter 2630: loss 1.0853, time 218.01ms, mfu 1.66%\n",
            "iter 2640: loss 1.0531, time 215.81ms, mfu 1.67%\n",
            "iter 2650: loss 1.0274, time 218.34ms, mfu 1.67%\n",
            "iter 2660: loss 1.0603, time 216.57ms, mfu 1.68%\n",
            "iter 2670: loss 1.0643, time 218.54ms, mfu 1.68%\n",
            "iter 2680: loss 1.0933, time 217.38ms, mfu 1.69%\n",
            "iter 2690: loss 1.0530, time 216.25ms, mfu 1.69%\n",
            "iter 2700: loss 1.0779, time 217.91ms, mfu 1.69%\n",
            "iter 2710: loss 1.0458, time 215.17ms, mfu 1.70%\n",
            "iter 2720: loss 1.0199, time 214.80ms, mfu 1.70%\n",
            "iter 2730: loss 1.0259, time 217.07ms, mfu 1.70%\n",
            "iter 2740: loss 1.0780, time 217.58ms, mfu 1.70%\n",
            "step 2750: train loss 0.9336, val loss 2.4148\n",
            "iter 2750: loss 1.0299, time 30738.85ms, mfu 1.53%\n",
            "iter 2760: loss 1.0352, time 218.33ms, mfu 1.55%\n",
            "iter 2770: loss 1.0627, time 217.10ms, mfu 1.57%\n",
            "iter 2780: loss 1.0398, time 217.88ms, mfu 1.58%\n",
            "iter 2790: loss 1.0372, time 218.09ms, mfu 1.60%\n",
            "iter 2800: loss 1.0217, time 215.89ms, mfu 1.61%\n",
            "iter 2810: loss 1.0280, time 214.70ms, mfu 1.62%\n",
            "iter 2820: loss 1.0069, time 219.49ms, mfu 1.63%\n",
            "iter 2830: loss 1.0317, time 218.05ms, mfu 1.64%\n",
            "iter 2840: loss 1.0482, time 215.95ms, mfu 1.65%\n",
            "iter 2850: loss 1.0415, time 218.30ms, mfu 1.65%\n",
            "iter 2860: loss 1.0239, time 214.38ms, mfu 1.66%\n",
            "iter 2870: loss 1.0459, time 217.73ms, mfu 1.67%\n",
            "iter 2880: loss 1.0313, time 216.06ms, mfu 1.67%\n",
            "iter 2890: loss 1.0199, time 219.17ms, mfu 1.68%\n",
            "iter 2900: loss 1.0401, time 216.96ms, mfu 1.68%\n",
            "iter 2910: loss 1.0611, time 215.10ms, mfu 1.69%\n",
            "iter 2920: loss 1.0362, time 214.68ms, mfu 1.69%\n",
            "iter 2930: loss 1.0019, time 219.89ms, mfu 1.69%\n",
            "iter 2940: loss 0.9898, time 218.24ms, mfu 1.69%\n",
            "iter 2950: loss 1.0166, time 217.48ms, mfu 1.69%\n",
            "iter 2960: loss 1.0163, time 217.94ms, mfu 1.70%\n",
            "iter 2970: loss 1.0245, time 218.86ms, mfu 1.70%\n",
            "iter 2980: loss 1.0285, time 217.93ms, mfu 1.70%\n",
            "iter 2990: loss 1.0328, time 220.39ms, mfu 1.70%\n",
            "step 3000: train loss 0.9104, val loss 2.4409\n",
            "iter 3000: loss 1.0075, time 30716.34ms, mfu 1.53%\n",
            "iter 3010: loss 1.0094, time 217.62ms, mfu 1.55%\n",
            "iter 3020: loss 1.0574, time 215.81ms, mfu 1.57%\n",
            "iter 3030: loss 1.0436, time 216.99ms, mfu 1.58%\n",
            "iter 3040: loss 1.0005, time 214.30ms, mfu 1.60%\n",
            "iter 3050: loss 1.0072, time 217.91ms, mfu 1.61%\n",
            "iter 3060: loss 0.9876, time 215.55ms, mfu 1.62%\n",
            "iter 3070: loss 1.0120, time 218.32ms, mfu 1.63%\n",
            "iter 3080: loss 1.0295, time 217.79ms, mfu 1.64%\n",
            "iter 3090: loss 1.0470, time 218.95ms, mfu 1.64%\n",
            "iter 3100: loss 1.0002, time 216.00ms, mfu 1.65%\n",
            "iter 3110: loss 0.9974, time 218.54ms, mfu 1.66%\n",
            "iter 3120: loss 1.0332, time 217.62ms, mfu 1.66%\n",
            "iter 3130: loss 0.9834, time 218.22ms, mfu 1.67%\n",
            "iter 3140: loss 1.0300, time 218.57ms, mfu 1.67%\n",
            "iter 3150: loss 1.0253, time 219.16ms, mfu 1.67%\n",
            "iter 3160: loss 0.9691, time 216.62ms, mfu 1.68%\n",
            "iter 3170: loss 1.0190, time 219.08ms, mfu 1.68%\n",
            "iter 3180: loss 1.0207, time 214.62ms, mfu 1.69%\n",
            "iter 3190: loss 0.9917, time 217.15ms, mfu 1.69%\n",
            "iter 3200: loss 0.9929, time 217.83ms, mfu 1.69%\n",
            "iter 3210: loss 1.0100, time 215.63ms, mfu 1.70%\n",
            "iter 3220: loss 1.0168, time 218.00ms, mfu 1.70%\n",
            "iter 3230: loss 1.0090, time 217.36ms, mfu 1.70%\n",
            "iter 3240: loss 1.0200, time 215.73ms, mfu 1.70%\n",
            "step 3250: train loss 0.8942, val loss 2.4340\n",
            "iter 3250: loss 1.0287, time 30737.92ms, mfu 1.53%\n",
            "iter 3260: loss 1.0091, time 216.01ms, mfu 1.55%\n",
            "iter 3270: loss 0.9755, time 215.08ms, mfu 1.57%\n",
            "iter 3280: loss 1.0163, time 217.64ms, mfu 1.59%\n",
            "iter 3290: loss 1.0188, time 218.39ms, mfu 1.60%\n",
            "iter 3300: loss 1.0140, time 219.75ms, mfu 1.61%\n",
            "iter 3310: loss 0.9791, time 217.92ms, mfu 1.62%\n",
            "iter 3320: loss 1.0044, time 217.12ms, mfu 1.63%\n",
            "iter 3330: loss 0.9940, time 215.16ms, mfu 1.64%\n",
            "iter 3340: loss 1.0181, time 218.94ms, mfu 1.64%\n",
            "iter 3350: loss 1.0262, time 216.39ms, mfu 1.65%\n",
            "iter 3360: loss 0.9962, time 218.13ms, mfu 1.66%\n",
            "iter 3370: loss 1.0196, time 216.28ms, mfu 1.67%\n",
            "iter 3380: loss 1.0098, time 220.23ms, mfu 1.67%\n",
            "iter 3390: loss 1.0159, time 216.66ms, mfu 1.67%\n",
            "iter 3400: loss 0.9949, time 219.75ms, mfu 1.68%\n",
            "iter 3410: loss 1.0214, time 215.17ms, mfu 1.68%\n",
            "iter 3420: loss 1.0200, time 217.22ms, mfu 1.68%\n",
            "iter 3430: loss 1.0138, time 215.32ms, mfu 1.69%\n",
            "iter 3440: loss 1.0082, time 217.91ms, mfu 1.69%\n",
            "iter 3450: loss 0.9954, time 217.44ms, mfu 1.69%\n",
            "iter 3460: loss 0.9832, time 215.25ms, mfu 1.70%\n",
            "iter 3470: loss 0.9901, time 220.71ms, mfu 1.70%\n",
            "iter 3480: loss 1.0191, time 214.80ms, mfu 1.70%\n",
            "iter 3490: loss 1.0201, time 218.02ms, mfu 1.70%\n",
            "step 3500: train loss 0.8794, val loss 2.4241\n",
            "iter 3500: loss 0.9858, time 30718.48ms, mfu 1.53%\n",
            "iter 3510: loss 1.0011, time 217.27ms, mfu 1.55%\n",
            "iter 3520: loss 0.9964, time 215.58ms, mfu 1.57%\n",
            "iter 3530: loss 1.0245, time 218.75ms, mfu 1.58%\n",
            "iter 3540: loss 1.0225, time 217.15ms, mfu 1.60%\n",
            "iter 3550: loss 1.0066, time 220.27ms, mfu 1.61%\n",
            "iter 3560: loss 1.0051, time 218.88ms, mfu 1.62%\n",
            "iter 3570: loss 0.9979, time 218.41ms, mfu 1.62%\n",
            "iter 3580: loss 0.9761, time 214.38ms, mfu 1.64%\n",
            "iter 3590: loss 1.0021, time 218.56ms, mfu 1.64%\n",
            "iter 3600: loss 0.9762, time 219.86ms, mfu 1.65%\n",
            "iter 3610: loss 0.9645, time 215.59ms, mfu 1.66%\n",
            "iter 3620: loss 0.9768, time 220.14ms, mfu 1.66%\n",
            "iter 3630: loss 0.9898, time 217.22ms, mfu 1.67%\n",
            "iter 3640: loss 1.0338, time 216.56ms, mfu 1.67%\n",
            "iter 3650: loss 1.0062, time 214.94ms, mfu 1.68%\n",
            "iter 3660: loss 0.9883, time 217.92ms, mfu 1.68%\n",
            "iter 3670: loss 0.9926, time 215.49ms, mfu 1.69%\n",
            "iter 3680: loss 0.9935, time 218.12ms, mfu 1.69%\n",
            "iter 3690: loss 0.9985, time 216.90ms, mfu 1.69%\n",
            "iter 3700: loss 0.9887, time 218.25ms, mfu 1.69%\n",
            "iter 3710: loss 1.0039, time 219.43ms, mfu 1.69%\n",
            "iter 3720: loss 1.0202, time 216.42ms, mfu 1.70%\n",
            "iter 3730: loss 0.9602, time 217.03ms, mfu 1.70%\n",
            "iter 3740: loss 0.9780, time 218.54ms, mfu 1.70%\n",
            "step 3750: train loss 0.8654, val loss 2.4388\n",
            "iter 3750: loss 1.0000, time 30833.01ms, mfu 1.53%\n",
            "iter 3760: loss 0.9816, time 217.49ms, mfu 1.55%\n",
            "iter 3770: loss 0.9868, time 217.95ms, mfu 1.57%\n",
            "iter 3780: loss 0.9916, time 217.20ms, mfu 1.58%\n",
            "iter 3790: loss 0.9706, time 218.16ms, mfu 1.59%\n",
            "iter 3800: loss 0.9425, time 218.56ms, mfu 1.60%\n",
            "iter 3810: loss 0.9817, time 220.37ms, mfu 1.61%\n",
            "iter 3820: loss 0.9924, time 215.93ms, mfu 1.63%\n",
            "iter 3830: loss 1.0018, time 218.53ms, mfu 1.63%\n",
            "iter 3840: loss 0.9787, time 213.83ms, mfu 1.64%\n",
            "iter 3850: loss 1.0158, time 218.05ms, mfu 1.65%\n",
            "iter 3860: loss 1.0123, time 215.04ms, mfu 1.66%\n",
            "iter 3870: loss 0.9821, time 214.71ms, mfu 1.67%\n",
            "iter 3880: loss 0.9577, time 216.65ms, mfu 1.67%\n",
            "iter 3890: loss 0.9910, time 217.03ms, mfu 1.68%\n",
            "iter 3900: loss 0.9724, time 219.58ms, mfu 1.68%\n",
            "iter 3910: loss 1.0151, time 215.25ms, mfu 1.68%\n",
            "iter 3920: loss 0.9820, time 218.45ms, mfu 1.69%\n",
            "iter 3930: loss 0.9562, time 216.77ms, mfu 1.69%\n",
            "iter 3940: loss 0.9717, time 217.21ms, mfu 1.69%\n",
            "iter 3950: loss 0.9768, time 216.82ms, mfu 1.70%\n",
            "iter 3960: loss 0.9823, time 217.09ms, mfu 1.70%\n",
            "iter 3970: loss 0.9700, time 214.42ms, mfu 1.70%\n",
            "iter 3980: loss 0.9623, time 216.14ms, mfu 1.70%\n",
            "iter 3990: loss 0.9997, time 218.72ms, mfu 1.70%\n",
            "step 4000: train loss 0.8553, val loss 2.4406\n",
            "iter 4000: loss 0.9463, time 30668.37ms, mfu 1.54%\n",
            "iter 4010: loss 0.9888, time 217.39ms, mfu 1.55%\n",
            "iter 4020: loss 0.9666, time 215.28ms, mfu 1.57%\n",
            "iter 4030: loss 0.9795, time 217.64ms, mfu 1.59%\n",
            "iter 4040: loss 0.9541, time 216.02ms, mfu 1.60%\n",
            "iter 4050: loss 0.9916, time 216.44ms, mfu 1.61%\n",
            "iter 4060: loss 0.9592, time 217.52ms, mfu 1.62%\n",
            "iter 4070: loss 0.9536, time 218.86ms, mfu 1.63%\n",
            "iter 4080: loss 0.9877, time 216.82ms, mfu 1.64%\n",
            "iter 4090: loss 0.9814, time 220.58ms, mfu 1.64%\n",
            "iter 4100: loss 0.9940, time 216.18ms, mfu 1.65%\n",
            "iter 4110: loss 0.9535, time 221.33ms, mfu 1.66%\n",
            "iter 4120: loss 0.9951, time 215.68ms, mfu 1.66%\n",
            "iter 4130: loss 0.9763, time 217.99ms, mfu 1.67%\n",
            "iter 4140: loss 0.9700, time 216.39ms, mfu 1.67%\n",
            "iter 4150: loss 0.9709, time 217.61ms, mfu 1.68%\n",
            "iter 4160: loss 0.9959, time 214.92ms, mfu 1.68%\n",
            "iter 4170: loss 0.9831, time 218.24ms, mfu 1.69%\n",
            "iter 4180: loss 0.9564, time 215.04ms, mfu 1.69%\n",
            "iter 4190: loss 0.9799, time 219.02ms, mfu 1.69%\n",
            "iter 4200: loss 0.9468, time 215.08ms, mfu 1.70%\n",
            "iter 4210: loss 0.9733, time 219.81ms, mfu 1.70%\n",
            "iter 4220: loss 0.9926, time 215.86ms, mfu 1.70%\n",
            "iter 4230: loss 0.9640, time 219.27ms, mfu 1.70%\n",
            "iter 4240: loss 0.9585, time 218.29ms, mfu 1.70%\n",
            "step 4250: train loss 0.8446, val loss 2.4587\n",
            "iter 4250: loss 0.9391, time 30712.79ms, mfu 1.53%\n",
            "iter 4260: loss 0.9758, time 217.93ms, mfu 1.55%\n",
            "iter 4270: loss 0.9575, time 215.15ms, mfu 1.57%\n",
            "iter 4280: loss 0.9433, time 215.03ms, mfu 1.58%\n",
            "iter 4290: loss 1.0031, time 218.66ms, mfu 1.60%\n",
            "iter 4300: loss 0.9800, time 215.85ms, mfu 1.61%\n",
            "iter 4310: loss 0.9877, time 218.95ms, mfu 1.62%\n",
            "iter 4320: loss 0.9911, time 215.00ms, mfu 1.63%\n",
            "iter 4330: loss 0.9645, time 218.61ms, mfu 1.64%\n",
            "iter 4340: loss 0.9084, time 214.71ms, mfu 1.65%\n",
            "iter 4350: loss 0.9525, time 217.26ms, mfu 1.66%\n",
            "iter 4360: loss 0.9856, time 214.63ms, mfu 1.66%\n",
            "iter 4370: loss 0.9803, time 217.84ms, mfu 1.67%\n",
            "iter 4380: loss 0.9844, time 217.36ms, mfu 1.67%\n",
            "iter 4390: loss 0.9662, time 218.40ms, mfu 1.68%\n",
            "iter 4400: loss 0.9603, time 218.52ms, mfu 1.68%\n",
            "iter 4410: loss 0.9702, time 216.63ms, mfu 1.68%\n",
            "iter 4420: loss 0.9436, time 218.49ms, mfu 1.69%\n",
            "iter 4430: loss 0.9548, time 217.04ms, mfu 1.69%\n",
            "iter 4440: loss 0.9593, time 218.48ms, mfu 1.69%\n",
            "iter 4450: loss 1.0048, time 218.79ms, mfu 1.69%\n",
            "iter 4460: loss 1.0018, time 216.97ms, mfu 1.70%\n",
            "iter 4470: loss 0.9701, time 219.32ms, mfu 1.70%\n",
            "iter 4480: loss 0.9759, time 215.22ms, mfu 1.70%\n",
            "iter 4490: loss 0.9450, time 221.18ms, mfu 1.70%\n",
            "step 4500: train loss 0.8404, val loss 2.4654\n",
            "iter 4500: loss 0.9737, time 30747.24ms, mfu 1.53%\n",
            "iter 4510: loss 0.9927, time 216.44ms, mfu 1.55%\n",
            "iter 4520: loss 0.9487, time 217.85ms, mfu 1.57%\n",
            "iter 4530: loss 0.9579, time 217.22ms, mfu 1.58%\n",
            "iter 4540: loss 0.9677, time 214.76ms, mfu 1.60%\n",
            "iter 4550: loss 1.0104, time 218.43ms, mfu 1.61%\n",
            "iter 4560: loss 0.9615, time 218.81ms, mfu 1.62%\n",
            "iter 4570: loss 0.9731, time 218.14ms, mfu 1.63%\n",
            "iter 4580: loss 0.9505, time 219.33ms, mfu 1.63%\n",
            "iter 4590: loss 0.9619, time 219.76ms, mfu 1.64%\n",
            "iter 4600: loss 0.9350, time 216.98ms, mfu 1.65%\n",
            "iter 4610: loss 0.9642, time 218.66ms, mfu 1.65%\n",
            "iter 4620: loss 0.9270, time 216.11ms, mfu 1.66%\n",
            "iter 4630: loss 0.9491, time 214.56ms, mfu 1.67%\n",
            "iter 4640: loss 0.9422, time 214.98ms, mfu 1.68%\n",
            "iter 4650: loss 0.9649, time 218.01ms, mfu 1.68%\n",
            "iter 4660: loss 0.9256, time 215.28ms, mfu 1.68%\n",
            "iter 4670: loss 0.9614, time 221.19ms, mfu 1.68%\n",
            "iter 4680: loss 0.9578, time 219.06ms, mfu 1.69%\n",
            "iter 4690: loss 0.9604, time 218.62ms, mfu 1.69%\n",
            "iter 4700: loss 0.9961, time 216.04ms, mfu 1.69%\n",
            "iter 4710: loss 0.9696, time 219.51ms, mfu 1.69%\n",
            "iter 4720: loss 0.9925, time 214.86ms, mfu 1.70%\n",
            "iter 4730: loss 0.9596, time 218.03ms, mfu 1.70%\n",
            "iter 4740: loss 0.9099, time 215.53ms, mfu 1.70%\n",
            "step 4750: train loss 0.8387, val loss 2.4364\n",
            "iter 4750: loss 0.9833, time 30770.18ms, mfu 1.53%\n",
            "iter 4760: loss 0.9772, time 217.80ms, mfu 1.55%\n",
            "iter 4770: loss 0.9461, time 218.57ms, mfu 1.57%\n",
            "iter 4780: loss 0.9393, time 218.59ms, mfu 1.58%\n",
            "iter 4790: loss 0.9390, time 216.92ms, mfu 1.59%\n",
            "iter 4800: loss 0.9660, time 217.69ms, mfu 1.61%\n",
            "iter 4810: loss 0.9365, time 220.67ms, mfu 1.61%\n",
            "iter 4820: loss 0.9299, time 218.95ms, mfu 1.62%\n",
            "iter 4830: loss 0.9632, time 214.93ms, mfu 1.63%\n",
            "iter 4840: loss 0.9681, time 218.76ms, mfu 1.64%\n",
            "iter 4850: loss 0.9519, time 216.92ms, mfu 1.65%\n",
            "iter 4860: loss 0.9576, time 219.81ms, mfu 1.65%\n",
            "iter 4870: loss 0.9495, time 215.57ms, mfu 1.66%\n",
            "iter 4880: loss 0.9456, time 217.11ms, mfu 1.67%\n",
            "iter 4890: loss 0.9430, time 217.93ms, mfu 1.67%\n",
            "iter 4900: loss 0.9810, time 218.30ms, mfu 1.68%\n",
            "iter 4910: loss 0.9380, time 218.82ms, mfu 1.68%\n",
            "iter 4920: loss 0.9373, time 218.60ms, mfu 1.68%\n",
            "iter 4930: loss 0.9506, time 218.19ms, mfu 1.68%\n",
            "iter 4940: loss 0.9684, time 219.42ms, mfu 1.69%\n",
            "iter 4950: loss 0.9652, time 219.49ms, mfu 1.69%\n",
            "iter 4960: loss 0.9599, time 215.90ms, mfu 1.69%\n",
            "iter 4970: loss 0.9670, time 219.37ms, mfu 1.69%\n",
            "iter 4980: loss 0.9732, time 216.50ms, mfu 1.69%\n",
            "iter 4990: loss 0.9596, time 217.96ms, mfu 1.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(trump_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wce0bFyo-C4q",
        "outputId": "88dc79d3-1318-4f48-bfc7-bd39834e939d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta from meta.pkl...\n",
            "Generated Example 1: \n",
            "\n",
            "\n",
            "But we will make our country so strong. We lose our country great again. We will win.\n",
            "We will make it so strong. We will go and we can take care of our vets.\n",
            "So we cant talk about it.\n",
            "\n",
            "\n",
            "And we use the strategic and we cant have a big tax in the world. We are going, you know, we have to use them in a woman and we owe $11 trillion.\n",
            "\n",
            "\n",
            "We all know whats going on fast. And we have to go out to be. We will tell you, we have a great learner too win and we won the truth. We got to Iraq. We will make\n",
            "\n",
            "Generated Example 2: \n",
            "\n",
            "\n",
            "Im going to bring back our money back in. A lot of these guys are crooking for Trump. The drugs that wanted to have to keep them in the United States. In fact, which is going to happen to have to say, \"We will continue that we cant have with the people of interests. We have to do it, so much tax. Were going to have to have to use something, folks. We have now what were going to do it.\n",
            "I have to do it.\n",
            "You know, when you remember this  I have to be cross than anything.\n",
            "And you know, theyre\n",
            "\n",
            "Generated Example 3: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "So  you know, if I dont know what a lot of things are never going to get along with a lot of the great deals in the world with the China. Right? Its me.\n",
            "\n",
            "\n",
            "\n",
            "The brand is that will be in the worst. When I love this country because were going to do solved. Were going to do great with evangelicals and theyre tougher because I am going to give it back to your children. Theyre going to find it. Now, theyre going to be so wonderful. The border and the budget things that are going to get a fr\n",
            "\n",
            "Generated Example 4: \n",
            "\n",
            "exico is going to be turned of a replace. Were going to bring in it back. Were going to make America great again.\n",
            "Were going to make America great again. Our country is going to be so thanking again. Were going to make America great again. Were going to make America great again.\n",
            "Were going to protect your Medicare. Were going to win it. Were going to make America great again.\n",
            "So were going to protect your country rich.\n",
            "And were going to make America great again. Were going to win the \n",
            "\n",
            "Generated Example 5: \n",
            "\n",
            "I have a tremendous deal with the money. I dont know, somebodys like that are so bad stuff. Hes going to go on really rich against right now.\n",
            "And hes going to be so good. Hes not going to be a real very nice guy.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "So it can't let Mexico is going to be very speaking for our country. When people say I dont have anybody that they are incompetent and they want to do it.\n",
            "\n",
            "\n",
            "Thats the greatest people of the Hispanics. Theyre all over the planes.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Well, what I just came in from t\n",
            "\n",
            "Generated Example 6: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Thats what it is in the entime I have a wonderful guy, and I just say that has to be a very exciting guy.\n",
            "When I go through them a very because Im going to find over a lot more than ever before you need a lot of time Im the only one that I didnt know if I win it will be just so important. Im self-funding my campaign.\n",
            "I understand thats not going to happen.\n",
            "And were going to get so much smart problems all the smartest guy  I have a closervative nice guy, I mean, hes a chunk people of\n",
            "\n",
            "Generated Example 7: \n",
            "\n",
            "\n",
            "\n",
            "I said, \"No, no, no, they dont want to destroy our military rich. Theyre allower talking about their is they dont even know what they replace to do. What are we? What were doing? We devaluing that? We can do it.\n",
            "\n",
            "\n",
            "We can Iowa are going to bring it back the wall. I want to save your military in the world and our jobs.\n",
            "\n",
            "\n",
            "And, always left the veterans are talking about everybody. And I think were going to make America great again.\n",
            "We're going to be our country so much, much more than that we\n",
            "\n",
            "Generated Example 8: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We need to know what happening. We have a military that we have to trade deals. We have to talk to happen and we have to appreciate it.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We need to know if we have to tell you one of them. We have to put the crowds are going to win in Iraq. We have to take care of our country rich. We will make our country so great again. We will make our country great.\n",
            "So we are going to bring it back, Iowa is than any more people. We will make our military so strong.\n",
            "And we will never ever be tal\n",
            "\n",
            "Generated Example 9: \n",
            "\n",
            "You dont get him all the greatest guys with the highest thing we have to do with one. And theyre the greatest cetera. But they are so they are going to pay for many decades for the education of the tubes that vote did a lot of press in the chWorld Trade Center  not allowed to let them because I see the best locations. So this is is a good guy.\n",
            "If I win, its very interesting, and California for two many of the other great country because I love the plan to stop it. I have spent to look at the\n",
            "\n",
            "Generated Example 10: \n",
            "\n",
            "And hes going to say \"Im going to win.\" Im going to run.\" And the problem. Because Ill tell you what, its going to be a lot of things but I was so sad. Now, by the way, thats so bad.\n",
            "I want to talk about that. I want to tell you.\n",
            "\n",
            "\n",
            "But when it does its the way its not going to be tire. Its so easy. Its a great guy for a long time. Weve got to look at Mexico. Were going to have a lot of money back the same impact in Iowa. I love the people of the world. I think then you cant get to h\n",
            "\n"
          ]
        }
      ]
    }
  ]
}